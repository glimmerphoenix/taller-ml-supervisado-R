# Introducción

Existen muchas definiciones complementarias sobre la Inteligencia Artificial (IA), cada una
reflejo de un modo diferente de aproximarse al complejo problema de crear máquinas que puedan
imitar la inteligencia humana [@russell2010]. Por ejemplo, en el artículo de Wikipedia
sobre IA se define de la siguiente forma [enlace](https://es.wikipedia.org/wiki/Inteligencia_artificial):

> La inteligencia artificial, abreviado como IA, en el contexto de las ciencias de la 
> computación, es una disciplina y un conjunto de capacidades cognoscitivas e intelectuales 
> expresadas por sistemas informáticos o combinaciones de algoritmos cuyo propósito es la 
> creación de máquinas que imiten la inteligencia humana. 

En 1950, Alan Turing propuso el llamado **Test de Turing** para proporcionar una definción
operativa satisfactoria sobre "inteligencia". En teoría, un computador supera el test si un
interrogador humano, después de plantear varias preguntas, no es capaz de discernir si las
respuestas escritas provienen de una persona o de un computador. Para poder superar el test,
un computador necesitaría incluir muchas habilidades. Aunque el test original no lo contemplaba,
el llamado **Test de Turing total** incorpora una señal de vídeo para que el interrogador pueda
evaluar las habilidades de percepción y reacción físicas del sujeto. Entre otros aspectos, esto
implica contar con habilidades como:

- *Procesamiento de lenguaje natural* (NLP), que permite a una máquina entender y comunicarse
correctamente, de forma verbal y/o por escrito en uno o varios idiomas.
- *Representación del conocimiento*, para almacenar lo aprendido de forma organizada.
- *Razonamiento automático* para emplear la información almacenada en responder a prenguntas y
extraer nuevas conclusiones.
- *Aprendizaje máquina*, para adaptarse a nuevas circunstancias y detectar y extrapolar 
patrones, a partir de datos previos que describan un problema o describiendo datos nuevos.
- *Visión computacional*, para percibir e interpretar imágenes y su contenido.
- *Robótica*, para maninpular objetos y poder moverse.

Por tanto, vemos que el **aprendizaje automático** (*machine learning* o ML en inglés) es una
rama de la IA que tiene por objetivo el desarrollo de técnicas y métodos para que las 
computadoras aprendan. Un **agente** inteligente aprende cuando es capaz de mejorar su
rendimiento a partir de la experiencia y de la información extraída de datos.

## Tipos de aprendizaje automático

Existe una taxonomía ampliamente aceptada para clasificar las diferentes aproximaciones para
resolver el problema del aprendizaje automático [@russell2010]:

- **Aprendizaje supervisado**: el agente observa algunos ejemplos de parejas de valores 
entrada-salida y aprende una función que mapea nuevas entradas a nuevos valores de salida.
- **Aprendizaje no supervisado**: el agente debe descubrir patrones o similitudes entre los
valores de entrada aunque no se le haya facilitado ningún tipo de información adicional.
- **Aprendizaje semisupervisado**: combina los dos anteriores, puesto que el agente recibe sólo 
unos pocos ejemplos y, a partir de ellos, tiene que hacer lo posible por descubrir patrones en
una colección de ejemplos no etiquetados o explicados.
- **Aprendizaje por refuerzo**: el método de aprendizaje implica que el agente reciba premios o
castigos. Por ejemplo, si un agente aprende a jugar al ajedrez y recibe dos puntos por haber
ganado una partida, deduce que lo ha hecho bien. Por contra, si un agente de conducción registra
un aviso de choque recibe una penalización para indicarle que lo ha hecho mal.

## Tareas de aprendizaje automático supervisado

En este taller práctico nos vamos a centrar en el aprendizaje supervisado, cuyo fin principal
es el de **predecir** el valor de **salida** que corresponde a una combinación concreta de 
valores descriptivos de entrada. Los dos tipos de tareas que se suelen abordar en aprendizaje
 automático supervisado son [@boehmke2019]:

- **Regresión**: el objetivo es predecir una **salida numérica**, cuyos valores se 
distribuyen a lo largo de un contínuo. Por ejemplo, un modelo que prediga el valor de venta de
una viviendia en función de varios datos descriptivos sobre la misma (superficie construida,
año de construcción, número de habitaciones, ubicación, etc.) es un modelo de regresión. 
No se debe confundir con el término "regresión lineal", que es un tipo específico de modelo 
de aprendizaje.

- **Clasificación**: el objetivo es predecir una **salida categórica** (etiqueta), que puede
tener dos posibles valores (*clasificación binaria*) o varios valores (*clasificación multinomial*).

![Ejemplo de salida de un modelo de predicción de valor de venta de viviendas en función de su año de construcción y superficie útil. Figura tomada de [@boehmke2019], sección 1.1.1.](img/intro-regression-problem.png){#fig-intro-regression-prob width=70%}

![Ejemplo un modelo de clasificación binaria, en función de tres parámetros descriptivos de entrada. Figura tomada de [@boehmke2019], sección 1.1.2.](img/classification_problem.png){#fig-intro-classif-prob width=90%}

## Ajuste de modelos

El **ajuste** de un modelo de aprendizaje automático comprende una *secuencia de pasos* o tareas
que permite que el algoritmo o modelo seleccionado aprenda a resolver un problema concreto. Es
decir, que aprenda a predecir un valor de salida en función de una combinación de valores de
entrada.

::: {.callout-note}
Dependiendo de si estamos hablando con personas que provienen del aprendizaje estadístico, las
ciencias de la computación o el reconocimiento de patrones, encontraremos términos diferentes
para designar a un mismo procedimiento. Por ejemplo, alguien de aprendizaje estadístico dirá que
va a *ajustar un modelo*, en el sentido de calcular, a partir de datos, el valor de los 
parámetros que lo determinan. Sin embargo, alguien de computación (aprendizaje automático) 
hablará más bien de *entrenar un algoritmo* con datos, lo que trae consigo elegir valores para sus parámetros.

La única diferencia, por tanto, es en qué parte del mismo proceso se pone el foco en cada caso. En muchas ocasiones, en este taller usaremos indistintamente los términos *ajuste* o *entremaiento* aplicados a un modelo o algoritmo.
:::

El proceso completo de ajuste o entrenamiento de un modelo se mostrará en la @fig-model-selec en el
@sec-supervised-ml. El primer paso en esta secuencia de ajuste de modelos siempre es la 
**división del conjunto de datos** en dos grupos [@boehmke2019]:

- Datos de **entrenamiento** (*training*): son los datos que vamos a emplear para que el algoritmo/modelo
aprenda a resolver el problema. Entre otros aspectos, permiten elegir las variables o entradas que vamos
a utilizar, calcular valores óptimos para los *hiperparámetros* del modelo, comparar el rendimiento de
varios modelos y el resto de acciones que conducen a elegir un modelo final.

- Datos de **prueba**: una vez que hemos seleccionado un modelo final, estos datos sirven para obtener
una *estimación no sesgada* del rendimiento de nuestro modelo cuando se enfrenta a datos que no ha visto
previamente, lo que denominamos **error de generalización**.

![División básica de datos en grupos de entrenamiento y prueba (training/testing)](img/training-testing.png){#fig-train-test width=75%}

::: {.callout-important}
Es de vital importancia garantizar que los datos de prueba (testing) nunca se usan durante el entrenamiento
del modelo/algoritmo. De otro modo, la selección del modelo final podría verse comprometida (introducción de
sesgo).
:::

Los porcentajes del total del datos que se recomendia usar para cada grupo suelen oscilar entre
60% (training) - 40% (testing), 70%-30% u 80%-20%. Usar más del 80% de los datos para entrenamiento sería
contraproducente, puesto que no tendríamos suficientes datos como para hacer una valoración adecuada del
modelo y, además, el ajuste encajaría demasiado en los datos observados y perdería capacidad de generalización
(**sobreajuste**, *overfitting* en inglés). Por contra, emplear demasiados datos en probar el modelo (>40%)
implicaría no tener suficientes datos como para ajustar sus parámetros de manera robusta.

No obstante, existen casos particulares. Si tenemos un dataset de gran tamaño, entonces no tiene demasiado
sentido crear un conjunto de entrenamiento muy grande, ya que la mayoría de los algoritmos de aprendizaje
automático (exceptuando, por ejemplo, los modelos basados en aprendizaje profundo más recientes) no obtienen
ningún beneficio por utilizar muchos datos para entrenarlos. Por otro lado, si $n$ es el tamaño de nuestro
dataset y $p$ el número de variables o *features* de entrada del modelo, cuando $p>>n$ sí se necesitan
grandes conjuntos de datos para estimar de manera fiable posibles patrones en los datos.

La pregunta es: ¿cómo podemos dividir nuestros datos en los conjuntos de entrenamiento y prueba? Para
resolver esta tarea se aplican técnicas de **muestreo de datos**. A continuación, se describen algunas
opciones habituales.

### Técnicas de muestreo

Normalmente, la división de los datos en estos dos grupos se realiza por medio de técnicas de muestreo. Las
dos técnicas más comúnmente aplicadas son:

- **Muestreo aleatorio simple**: es la forma más sencilla, consiste en tomar aleatoriamente muestras de
los datos hasta llenar cada uno de los grupos. La gran desventaja de este método es que no tiene en cuenta
distribuciones específicas de los datos en el conjunto original (por ejemplo, para preservar la proporción
de datos presentes en cada categoría). Un ejemplo lo podemos ver en la @fig-random-sampling-train-test.

- **Muestreo estratificado**: se utiliza cuando necesitamos controlar explícitamente que nuestros conjuntos
de entrenamiento y de prueba tengan distribuciones de valores similares a las del conjunto de datos original.
Podemos ver un ejemplo en la [Sec. 2.2.2](https://bradleyboehmke.github.io/HOML/process.html#stratified) de
[@boehmke2019].

![El muestreo aleatorio simple no tiene en cuenta la proporción de datos existente en el conjunto original. En este caso, después del muestreo el conjunto de entrenamiento mantiene aproximadamente estas proporciones, pero el de prueba no. Figura tomada de [@raschka2020modeleval].](img/random-sampling-train-test.png){#fig-random-sampling-train-test width=95%}

### Particionado de datos con R {.unnumbered}

En este apartado presentamos algunos ejemplos de implementación del proceso de particionado de datos
en R, empleando varios paquetes.

**Dataset: `AmesHousing`**: Información sobre ventas de propiedades inmobiliarias descrito en
[@deCock2011] y proporcionado por el paquete `AmesHousing` [@kuhn2020ames].

| Descripción              | Valor                                  |
|:-------------------------|:---------------------------------------|
| Tipo de problema         | Regresión (supervisada)                |
| Respuesta                | `Sales_Price`                          |
| Variables (*inputs*)     | 80                                     |
| Observaciones            | 2.930                                  |
| Objetivo                 | Predicción del precio de las viviendas |
| Más información          | `?AmesHousing::ames_raw`                |

: Descripción del dataset proporcionado por el paquete `AmesHousing`. {#tbl-desc-ames .striped .hover}

```{r}
#| label: chap1-packages
#| message: false

library(dplyr)    # Procesado y filtrado de datos
library(ggplot2)  # Gráficos
library(patchwork)

# Paquetes para preparación y modelado de datos
library(rsample)  # Técnicas de remuestreo
library(caret)    # Remuestreo y entrenamiento (Kuhn & Johnson)
library(h2o)      # Remuestreo y entrenamiento (H2O.ai)

# Configuración inicial de h2o
h2o.no_progress()  # Deshabilita barra de progreso de h2o
h2o.init()         # Lanza el cluster (local) de h2o
```

```{r}
#| label: ames-data

ames <- AmesHousing::make_ames()
ames.h2o <- as.h2o(ames)
```

#### Muestreo aleatorio simple {.unnumbered}

```{r}
#| label: random-sampling

# Con R Base
set.seed(123)  # Fijamos semilla, resultados reproducibles

index_1 <- sample(1:nrow(ames), round(nrow(ames) * 0.7))
train_1 <- ames[index_1, ]
test_1  <- ames[-index_1, ]

# Usando el paquete caret
set.seed(123)  # Resultados reproducibles
index_2 <- createDataPartition(ames$Sale_Price, p = 0.7, 
                               list = FALSE)
train_2 <- ames[index_2, ]
test_2  <- ames[-index_2, ]

# Usando el paquete rsample
set.seed(123)  # Resultados reproducibles
split_1  <- initial_split(ames, prop = 0.7)
train_3  <- training(split_1)
test_3   <- testing(split_1)

# Usando el paquete h2o
split_2 <- h2o.splitFrame(ames.h2o, ratios = 0.7, seed = 123)
train_4 <- split_2[[1]]
test_4  <- split_2[[2]]
```

La @fig-density-data-partitions muestra varios paneles donde podemos comprobar la distribución de valores 
de las diferenes particiones, respecto de los datos originales.

```{r}
#| label: fig-density-data-partitions
#| fig-cap: "Comparación de las distribuciones de valores obtenidas en las muestras. Rojo: datos originales; azul: partición de entrenamiento; verde: partición de prueba."
#| code-fold: true
#| code-summary: "Mostrar código"

compara_dens <- function(orig, train, test, var_name, main_title,
    name_orig = deparse(substitute(original)),
    name_train = deparse(substitute(train)),
    name_test = deparse(substitute(test))) {
    
    datos_combinados <- data.frame(
    valor = c(orig, train, test),
    grupo = c(rep(name_orig, length(orig)),
        rep(name_train, length(train)), 
        rep(name_test, length(test)))
    )

    p <- ggplot(datos_combinados, aes(x = valor, color = grupo)) +
    geom_density(alpha = 0.4) + # 'alpha' controla la transparencia (0 a 1)
    labs(
        title = main_title,
        subtitle = "Comparación train vs. test",
        x = var_name,
        y = "Densidad",
        color = "Variable"
    ) +
    theme_minimal() + theme(legend.position="none")

    return(p)
}

p1 <- compara_dens(ames$Sale_Price, train_1$Sale_Price, test_1$Sale_Price, "Sale_Price", "R base")
p2 <- compara_dens(ames$Sale_Price, train_2$Sale_Price, test_2$Sale_Price, "Sale_Price", "caret")
p3 <- compara_dens(ames$Sale_Price, train_3$Sale_Price, test_3$Sale_Price, "Sale_Price", "rsample")
p4 <- compara_dens(ames$Sale_Price, as.data.frame(train_4)$Sale_Price, 
                   as.data.frame(test_4)$Sale_Price, "Sale_Price", "h2o")

wrap_plots(p1, p2, p3, p4)
```

#### Muestreo estratificado {.unnumbered}

Consultar el ejemplo de la [Sec. 2.2.2](https://bradleyboehmke.github.io/HOML/process.html#stratified) en
[@boehmke2019], que explica cómo utilizar el paquete `rsample` para obtener un muestreo estratificado.

También es bastante directo obtener un [muestreo estratificado usando Tidyverse](https://rpubs.com/crossxwill/sampling_tidyverse).

### Desequilibrios en los datos

Otro aspecto que también tenemos que considerar es si existe algún tipo de **desequilibrio** entre las categorias
o la distribución de valores de nuestro conjunto de datos. Esto ocurre, por ejemplo, en datos etiquetados en los
que una categoría tiene muchas menos instancias que otras, como en los problemas de fraude o impagos. En un
dataset con dos categorías, `"fraudulento"` o `"legítimo"`, si la primera categoría tiene sólo un 1% del total 
de muestras y la segunda el 99% restante será difícil que los conjuntos de entrenamiento y prueba mantengan
esa proporción. Por otro lado, también será complicado que el modelo pueda aprender a identificar instancias de
la categoría minoritaria si tiene pocos ejemplos de los que poder aprender a reconocerla. Por ello, existen
varias técnicas para mitigar esta limitación:

- Submuestreo (*down-sampling*): reduce el tamaño de la clase más frecuente para que corresponda con la proporción
de instancias en la clase minoritaria. Se puede usar si el conjunto de datos original es grande.
- Sobremuestreo (*up-sampling*): si no hay muchos datos en el conjunto original, otra solución es utilizar
técnicas de remuestreo (como *bootstraping*) para generar nuevas muestras "artificiales" de la clase minoritaria.
- SMOTE: combina los dos métodos anteriores de forma innovadora.

Se pueden utilizar estos métodos en R mediante el argumento `sampling` en `caret::trainControl()` , así como
en otros paquetes (como `h2o`).

## Preparación de los datos

Antes de presentar algunos ejemplos de modelos supervisados de aprendizaje máquina y cómo podemos emplearlos
en R, conviene recordar que un paso previo imprescindible es preparar adecuadamente nuestros datos. Con demasiada
frecuencia, errores de codificación, ausencia de valores, escalas de representación demasiado amplias y muchos
otros problemas aparecen en nuestros datos y pueden malograr todos nuestros esfuerzos por conseguir ajustar
nuestros modelos y elegir la opción más adecuada.

Las tareas de limpieza y preparación de datos podrían llenar un taller o un curso entero por sí mismas.
Se invita al lector a consultar el [capítulo 3](https://bradleyboehmke.github.io/HOML/engineering.html) de
[@boehmke2019], así como las referencias [@kuhn2019featureeng] y [@zheng2018feature] para un tratamiento 
más detallado de muchas de las tareas de limpieza y preparación de datos necesarias antes de 
implementar y evaluar nuestros modelos de aprendizaje automático.