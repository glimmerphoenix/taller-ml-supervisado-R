[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Aprendizaje máquina supervisado con R",
    "section": "",
    "text": "Prefacio\nEn este taller exploramos aspectos prácticos del diseño, implementación y evaluación de modelos de aprendizaje automático supervisado. Estos modelos pueden aplicarse en múltiples problemas donde el objetivo es generar predicciones sobre nuevos datos, gracias a que el algoritmo ha sido previamente entrenado con un conjunto de datos que describe el problema a resolver.\nEste es un taller práctico que presenta ejemplos reales y comandos para entrenar, evaluar y aplicar modelos de aprendizaje automático supervisado con R. Además, junto a la explicación de los conceptos clave para entender este proceso también se ofrecen recomendaciones sobre buenas prácticas metodológicas para el ajuste (entrenamiento) de estos modelos, para comparar y elegir el algoritmo o modelo más adecuado para una aplicación específica, así como herramientas para interpretar mejor los resultados obtenidos.\nLos apuntes para este taller práctico se han realizado con Quarto, una herramienta para creación de documentación científica y programación literaria compatible con R y otros lenguajes de programación científica.",
    "crumbs": [
      "Prefacio"
    ]
  },
  {
    "objectID": "index.html#requisitos-previos",
    "href": "index.html#requisitos-previos",
    "title": "Aprendizaje máquina supervisado con R",
    "section": "Requisitos previos",
    "text": "Requisitos previos\nPara poder realizar los ejemplos inlcuidos en este taller necesitas tener instalado R y una IDE de desarrollo para este lenguaje. Se recomienda instalar RStudio o MS Visual Code como entorno de programación.\n\nInstalación de R.\nInstalación de RStudio.\n\nAdicionalmente, es necesario instalar una serie de paquetes R antes de ejecutar los ejemplos, para que todas las dependencias estén disponibles en nuestro sistema. Consulta el Apéndice ?sec-pkg-requirements para comprobar el listado de paquetes R necesarios.",
    "crumbs": [
      "Prefacio"
    ]
  },
  {
    "objectID": "01-introduccion.html",
    "href": "01-introduccion.html",
    "title": "1  Introducción",
    "section": "",
    "text": "1.1 Tipos de aprendizaje automático\nExisten muchas definiciones complementarias sobre la Inteligencia Artificial (IA), cada una reflejo de un modo diferente de aproximarse al complejo problema de crear máquinas que puedan imitar la inteligencia humana (Russell & Norvig, 2009). Por ejemplo, en el artículo de Wikipedia sobre IA se define de la siguiente forma enlace:\nEn 1950, Alan Turing propuso el llamado Test de Turing para proporcionar una definción operativa satisfactoria sobre “inteligencia”. En teoría, un computador supera el test si un interrogador humano, después de plantear varias preguntas, no es capaz de discernir si las respuestas escritas provienen de una persona o de un computador. Para poder superar el test, un computador necesitaría incluir muchas habilidades. Aunque el test original no lo contemplaba, el llamado Test de Turing total incorpora una señal de vídeo para que el interrogador pueda evaluar las habilidades de percepción y reacción físicas del sujeto. Entre otros aspectos, esto implica contar con habilidades como:\nPor tanto, vemos que el aprendizaje automático (machine learning o ML en inglés) es una rama de la IA que tiene por objetivo el desarrollo de técnicas y métodos para que las computadoras aprendan. Un agente inteligente aprende cuando es capaz de mejorar su rendimiento a partir de la experiencia y de la información extraída de datos.\nExiste una taxonomía ampliamente aceptada para clasificar las diferentes aproximaciones para resolver el problema del aprendizaje automático (Russell & Norvig, 2009):",
    "crumbs": [
      "Fundamentos",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "01-introduccion.html#tipos-de-aprendizaje-automático",
    "href": "01-introduccion.html#tipos-de-aprendizaje-automático",
    "title": "1  Introducción",
    "section": "",
    "text": "Aprendizaje supervisado: el agente observa algunos ejemplos de parejas de valores entrada-salida y aprende una función que mapea nuevas entradas a nuevos valores de salida.\nAprendizaje no supervisado: el agente debe descubrir patrones o similitudes entre los valores de entrada aunque no se le haya facilitado ningún tipo de información adicional.\nAprendizaje semisupervisado: combina los dos anteriores, puesto que el agente recibe sólo unos pocos ejemplos y, a partir de ellos, tiene que hacer lo posible por descubrir patrones en una colección de ejemplos no etiquetados o explicados.\nAprendizaje por refuerzo: el método de aprendizaje implica que el agente reciba premios o castigos. Por ejemplo, si un agente aprende a jugar al ajedrez y recibe dos puntos por haber ganado una partida, deduce que lo ha hecho bien. Por contra, si un agente de conducción registra un aviso de choque recibe una penalización para indicarle que lo ha hecho mal.",
    "crumbs": [
      "Fundamentos",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "01-introduccion.html#tareas-de-aprendizaje-automático-supervisado",
    "href": "01-introduccion.html#tareas-de-aprendizaje-automático-supervisado",
    "title": "1  Introducción",
    "section": "1.2 Tareas de aprendizaje automático supervisado",
    "text": "1.2 Tareas de aprendizaje automático supervisado\nEn este taller práctico nos vamos a centrar en el aprendizaje supervisado, cuyo fin principal es el de predecir el valor de salida que corresponde a una combinación concreta de valores descriptivos de entrada. Los dos tipos de tareas que se suelen abordar en aprendizaje automático supervisado son (Boehmke & Greenwell, 2019):\n\nRegresión: el objetivo es predecir una salida numérica, cuyos valores se distribuyen a lo largo de un contínuo. Por ejemplo, un modelo que prediga el valor de venta de una viviendia en función de varios datos descriptivos sobre la misma (superficie construida, año de construcción, número de habitaciones, ubicación, etc.) es un modelo de regresión. No se debe confundir con el término “regresión lineal”, que es un tipo específico de modelo de aprendizaje.\nClasificación: el objetivo es predecir una salida categórica (etiqueta), que puede tener dos posibles valores (clasificación binaria) o varios valores (clasificación multinomial).\n\n\n\n\n\n\n\nFigura 1.1: Ejemplo de salida de un modelo de predicción de valor de venta de viviendas en función de su año de construcción y superficie útil. Figura tomada de (Boehmke & Greenwell, 2019), sección 1.1.1.\n\n\n\n\n\n\n\n\n\nFigura 1.2: Ejemplo un modelo de clasificación binaria, en función de tres parámetros descriptivos de entrada. Figura tomada de (Boehmke & Greenwell, 2019), sección 1.1.2.",
    "crumbs": [
      "Fundamentos",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "01-introduccion.html#ajuste-de-modelos",
    "href": "01-introduccion.html#ajuste-de-modelos",
    "title": "1  Introducción",
    "section": "1.3 Ajuste de modelos",
    "text": "1.3 Ajuste de modelos\nEl ajuste de un modelo de aprendizaje automático comprende una secuencia de pasos o tareas que permite que el algoritmo o modelo seleccionado aprenda a resolver un problema concreto. Es decir, que aprenda a predecir un valor de salida en función de una combinación de valores de entrada.\n\n\n\n\n\n\nNota\n\n\n\nDependiendo de si estamos hablando con personas que provienen del aprendizaje estadístico, las ciencias de la computación o el reconocimiento de patrones, encontraremos términos diferentes para designar a un mismo procedimiento. Por ejemplo, alguien de aprendizaje estadístico dirá que va a ajustar un modelo, en el sentido de calcular, a partir de datos, el valor de los parámetros que lo determinan. Sin embargo, alguien de computación (aprendizaje automático) hablará más bien de entrenar un algoritmo con datos, lo que trae consigo elegir valores para sus parámetros.\nLa única diferencia, por tanto, es en qué parte del mismo proceso se pone el foco en cada caso. En muchas ocasiones, en este taller usaremos indistintamente los términos ajuste o entremaiento aplicados a un modelo o algoritmo.\n\n\nEl proceso completo de ajuste o entrenamiento de un modelo se mostrará en la Figura 2.1 en el Capítulo 2. El primer paso en esta secuencia de ajuste de modelos siempre es la división del conjunto de datos en dos grupos (Boehmke & Greenwell, 2019):\n\nDatos de entrenamiento (training): son los datos que vamos a emplear para que el algoritmo/modelo aprenda a resolver el problema. Entre otros aspectos, permiten elegir las variables o entradas que vamos a utilizar, calcular valores óptimos para los hiperparámetros del modelo, comparar el rendimiento de varios modelos y el resto de acciones que conducen a elegir un modelo final.\nDatos de prueba: una vez que hemos seleccionado un modelo final, estos datos sirven para obtener una estimación no sesgada del rendimiento de nuestro modelo cuando se enfrenta a datos que no ha visto previamente, lo que denominamos error de generalización.\n\n\n\n\n\n\n\nFigura 1.3: División básica de datos en grupos de entrenamiento y prueba (training/testing)\n\n\n\n\n\n\n\n\n\nImportante\n\n\n\nEs de vital importancia garantizar que los datos de prueba (testing) nunca se usan durante el entrenamiento del modelo/algoritmo. De otro modo, la selección del modelo final podría verse comprometida (introducción de sesgo).\n\n\nLos porcentajes del total del datos que se recomendia usar para cada grupo suelen oscilar entre 60% (training) - 40% (testing), 70%-30% u 80%-20%. Usar más del 80% de los datos para entrenamiento sería contraproducente, puesto que no tendríamos suficientes datos como para hacer una valoración adecuada del modelo y, además, el ajuste encajaría demasiado en los datos observados y perdería capacidad de generalización (sobreajuste, overfitting en inglés). Por contra, emplear demasiados datos en probar el modelo (&gt;40%) implicaría no tener suficientes datos como para ajustar sus parámetros de manera robusta.\nNo obstante, existen casos particulares. Si tenemos un dataset de gran tamaño, entonces no tiene demasiado sentido crear un conjunto de entrenamiento muy grande, ya que la mayoría de los algoritmos de aprendizaje automático (exceptuando, por ejemplo, los modelos basados en aprendizaje profundo más recientes) no obtienen ningún beneficio por utilizar muchos datos para entrenarlos. Por otro lado, si \\(n\\) es el tamaño de nuestro dataset y \\(p\\) el número de variables o features de entrada del modelo, cuando \\(p&gt;&gt;n\\) sí se necesitan grandes conjuntos de datos para estimar de manera fiable posibles patrones en los datos.\nLa pregunta es: ¿cómo podemos dividir nuestros datos en los conjuntos de entrenamiento y prueba? Para resolver esta tarea se aplican técnicas de muestreo de datos. A continuación, se describen algunas opciones habituales.\n\n1.3.1 Técnicas de muestreo\nNormalmente, la división de los datos en estos dos grupos se realiza por medio de técnicas de muestreo. Las dos técnicas más comúnmente aplicadas son:\n\nMuestreo aleatorio simple: es la forma más sencilla, consiste en tomar aleatoriamente muestras de los datos hasta llenar cada uno de los grupos. La gran desventaja de este método es que no tiene en cuenta distribuciones específicas de los datos en el conjunto original (por ejemplo, para preservar la proporción de datos presentes en cada categoría). Un ejemplo lo podemos ver en la Figura 1.4.\nMuestreo estratificado: se utiliza cuando necesitamos controlar explícitamente que nuestros conjuntos de entrenamiento y de prueba tengan distribuciones de valores similares a las del conjunto de datos original. Podemos ver un ejemplo en la Sec. 2.2.2 de (Boehmke & Greenwell, 2019).\n\n\n\n\n\n\n\nFigura 1.4: El muestreo aleatorio simple no tiene en cuenta la proporción de datos existente en el conjunto original. En este caso, después del muestreo el conjunto de entrenamiento mantiene aproximadamente estas proporciones, pero el de prueba no. Figura tomada de (Raschka, 2020).\n\n\n\n\n\nParticionado de datos con R\nEn este apartado presentamos algunos ejemplos de implementación del proceso de particionado de datos en R, empleando varios paquetes.\nDataset: AmesHousing: Información sobre ventas de propiedades inmobiliarias descrito en (De Cock, 2011) y proporcionado por el paquete AmesHousing (Kuhn, 2020).\n\n\n\nTabla 1.1: Descripción del dataset proporcionado por el paquete AmesHousing.\n\n\n\n\n\nDescripción\nValor\n\n\n\n\nTipo de problema\nRegresión (supervisada)\n\n\nRespuesta\nSales_Price, cuantitativa\n\n\nVariables (inputs)\n80\n\n\nObservaciones\n2.930\n\n\nObjetivo\nPredicción del precio de las viviendas\n\n\nMás información\n?AmesHousing::ames_raw\n\n\n\n\n\n\n\nlibrary(dplyr)    # Procesado y filtrado de datos\nlibrary(ggplot2)  # Gráficos\nlibrary(patchwork)\n\n# Paquetes para preparación y modelado de datos\nlibrary(rsample)  # Técnicas de remuestreo\nlibrary(caret)    # Remuestreo y entrenamiento (Kuhn & Johnson)\nlibrary(h2o)      # Remuestreo y entrenamiento (H2O.ai)\n\n# Configuración inicial de h2o\nh2o.no_progress()  # Deshabilita barra de progreso de h2o\nh2o.init()         # Lanza el cluster (local) de h2o\n\n Connection successful!\n\nR is connected to the H2O cluster: \n    H2O cluster uptime:         11 minutes 45 seconds \n    H2O cluster timezone:       Europe/Madrid \n    H2O data parsing timezone:  UTC \n    H2O cluster version:        3.44.0.3 \n    H2O cluster version age:    1 year, 11 months and 4 days \n    H2O cluster name:           H2O_started_from_R_jfelipe_hva959 \n    H2O cluster total nodes:    1 \n    H2O cluster total memory:   14.52 GB \n    H2O cluster total cores:    24 \n    H2O cluster allowed cores:  24 \n    H2O cluster healthy:        TRUE \n    H2O Connection ip:          localhost \n    H2O Connection port:        54321 \n    H2O Connection proxy:       NA \n    H2O Internal Security:      FALSE \n    R Version:                  R version 4.5.2 (2025-10-31) \n\n\nWarning in h2o.clusterInfo(): \nYour H2O cluster version is (1 year, 11 months and 4 days) old. There may be a newer version available.\nPlease download and install the latest version from: https://h2o-release.s3.amazonaws.com/h2o/latest_stable.html\n\n\n\names &lt;- AmesHousing::make_ames()\names.h2o &lt;- as.h2o(ames)\n\n\nMuestreo aleatorio simple\n\n# Con R Base\nset.seed(123)  # Fijamos semilla, resultados reproducibles\n\nindex_1 &lt;- sample(1:nrow(ames), round(nrow(ames) * 0.7))\ntrain_1 &lt;- ames[index_1, ]\ntest_1  &lt;- ames[-index_1, ]\n\n# Usando el paquete caret\nset.seed(123)  # Resultados reproducibles\nindex_2 &lt;- createDataPartition(ames$Sale_Price, p = 0.7, \n                               list = FALSE)\ntrain_2 &lt;- ames[index_2, ]\ntest_2  &lt;- ames[-index_2, ]\n\n# Usando el paquete rsample\nset.seed(123)  # Resultados reproducibles\nsplit_1  &lt;- initial_split(ames, prop = 0.7)\ntrain_3  &lt;- training(split_1)\ntest_3   &lt;- testing(split_1)\n\n# Usando el paquete h2o\nsplit_2 &lt;- h2o.splitFrame(ames.h2o, ratios = 0.7, seed = 123)\ntrain_4 &lt;- split_2[[1]]\ntest_4  &lt;- split_2[[2]]\n\nLa Figura 1.5 muestra varios paneles donde podemos comprobar la distribución de valores de las diferenes particiones, respecto de los datos originales.\n\n\nMostrar código\ncompara_dens &lt;- function(orig, train, test, var_name, main_title,\n    name_orig = deparse(substitute(original)),\n    name_train = deparse(substitute(train)),\n    name_test = deparse(substitute(test))) {\n    \n    datos_combinados &lt;- data.frame(\n    valor = c(orig, train, test),\n    grupo = c(rep(name_orig, length(orig)),\n        rep(name_train, length(train)), \n        rep(name_test, length(test)))\n    )\n\n    p &lt;- ggplot(datos_combinados, aes(x = valor, color = grupo)) +\n    geom_density(alpha = 0.4) + # 'alpha' controla la transparencia (0 a 1)\n    labs(\n        title = main_title,\n        subtitle = \"Comparación train vs. test\",\n        x = var_name,\n        y = \"Densidad\",\n        color = \"Variable\"\n    ) +\n    theme_minimal() + theme(legend.position=\"none\")\n\n    return(p)\n}\n\np1 &lt;- compara_dens(ames$Sale_Price, train_1$Sale_Price, test_1$Sale_Price, \"Sale_Price\", \"R base\")\np2 &lt;- compara_dens(ames$Sale_Price, train_2$Sale_Price, test_2$Sale_Price, \"Sale_Price\", \"caret\")\np3 &lt;- compara_dens(ames$Sale_Price, train_3$Sale_Price, test_3$Sale_Price, \"Sale_Price\", \"rsample\")\np4 &lt;- compara_dens(ames$Sale_Price, as.data.frame(train_4)$Sale_Price, \n                   as.data.frame(test_4)$Sale_Price, \"Sale_Price\", \"h2o\")\n\nwrap_plots(p1, p2, p3, p4)\n\n\n\n\n\n\n\n\nFigura 1.5: Comparación de las distribuciones de valores obtenidas en las muestras. Rojo: datos originales; azul: partición de entrenamiento; verde: partición de prueba.\n\n\n\n\n\n\n\nMuestreo estratificado\nConsultar el ejemplo de la Sec. 2.2.2 en (Boehmke & Greenwell, 2019), que explica cómo utilizar el paquete rsample para obtener un muestreo estratificado.\nTambién es bastante directo obtener un muestreo estratificado usando Tidyverse.\n\n\n\n1.3.2 Desequilibrios en los datos\nOtro aspecto que también tenemos que considerar es si existe algún tipo de desequilibrio entre las categorias o la distribución de valores de nuestro conjunto de datos. Esto ocurre, por ejemplo, en datos etiquetados en los que una categoría tiene muchas menos instancias que otras, como en los problemas de fraude o impagos. En un dataset con dos categorías, \"fraudulento\" o \"legítimo\", si la primera categoría tiene sólo un 1% del total de muestras y la segunda el 99% restante será difícil que los conjuntos de entrenamiento y prueba mantengan esa proporción. Por otro lado, también será complicado que el modelo pueda aprender a identificar instancias de la categoría minoritaria si tiene pocos ejemplos de los que poder aprender a reconocerla. Por ello, existen varias técnicas para mitigar esta limitación:\n\nSubmuestreo (down-sampling): reduce el tamaño de la clase más frecuente para que corresponda con la proporción de instancias en la clase minoritaria. Se puede usar si el conjunto de datos original es grande.\nSobremuestreo (up-sampling): si no hay muchos datos en el conjunto original, otra solución es utilizar técnicas de remuestreo (como bootstraping) para generar nuevas muestras “artificiales” de la clase minoritaria.\nSMOTE: combina los dos métodos anteriores de forma innovadora.\n\nSe pueden utilizar estos métodos en R mediante el argumento sampling en caret::trainControl() , así como en otros paquetes (como h2o).",
    "crumbs": [
      "Fundamentos",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "01-introduccion.html#preparación-de-los-datos",
    "href": "01-introduccion.html#preparación-de-los-datos",
    "title": "1  Introducción",
    "section": "1.4 Preparación de los datos",
    "text": "1.4 Preparación de los datos\nAntes de presentar algunos ejemplos de modelos supervisados de aprendizaje máquina y cómo podemos emplearlos en R, conviene recordar que un paso previo imprescindible es preparar adecuadamente nuestros datos. Con demasiada frecuencia, errores de codificación, ausencia de valores, escalas de representación demasiado amplias y muchos otros problemas aparecen en nuestros datos y pueden malograr todos nuestros esfuerzos por conseguir ajustar nuestros modelos y elegir la opción más adecuada.\nLas tareas de limpieza y preparación de datos podrían llenar un taller o un curso entero por sí mismas. Se invita al lector a consultar el capítulo 3 de (Boehmke & Greenwell, 2019), así como las referencias (Kuhn & Johnson, 2019) y (Zheng & Casari, 2018) para un tratamiento más detallado de muchas de las tareas de limpieza y preparación de datos necesarias antes de implementar y evaluar nuestros modelos de aprendizaje automático.\n\n\n\n\nBoehmke, B., & Greenwell, B. (2019). Hands-On Machine Learning with R (1.ª ed.). Chapman; Hall/CRC. https://doi.org/10.1201/9780367816377\n\n\nDe Cock, D. (2011). Ames, Iowa: Alternative to the Boston Housing Data as an End of Semester Regression Project. Journal of Statistics Education, 19(3). https://doi.org/10.1080/10691898.2011.11889627\n\n\nKuhn, M. (2020). AmesHousing: The Ames Iowa Housing Data. https://doi.org/10.32614/CRAN.package.AmesHousing\n\n\nKuhn, M., & Johnson, K. (2019). Feature Engineering and Selection: A Practical Approach for Predictive Models. Chapman & Hall/CRC. http://www.feat.engineering/\n\n\nRaschka, S. (2020). Model Evaluation, Model Selection, and Algorithm Selection in Machine Learning. https://arxiv.org/abs/1811.12808\n\n\nRussell, S., & Norvig, P. (2009). Artificial Intelligence: A Modern Approach (3.ª ed.). Prentice Hall Press.\n\n\nZheng, A., & Casari, A. (2018). FFeature Engineering for Machine Learning: Principles and Techniques for Data Scientists. O’Reilly Media, Inc. https://www.oreilly.com/library/view/feature-engineering-for/9781491953235/",
    "crumbs": [
      "Fundamentos",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "02-ml-supervisado.html",
    "href": "02-ml-supervisado.html",
    "title": "2  Aprendizaje máquina supervisado",
    "section": "",
    "text": "2.1 Parámetros e hiperparámetros\nComo hemos introducido en el tema anterior, el aprendizaje máquina supervisado es un proceso que implica entrenar a un algoritmo computacional para que resuelva un problema de predicción (regresión o clasificación), utilizando para ello un conjunto de datos. La Figura 2.1 muestra un esquema del proceso completo.\nAntes de profundizar un poco en todo este proceso, conviene puntualizar el significado de un par de términos que suelen provocar bastante confusión.\nDurante el proceso de aprendizaje, el algoritmo o modelo escogido puede ajustar el valor de ciertos parámetros a partir de la información extraída de los datos. Pensemos, por ejemplo, en un modelo de regresión lineal. Dicho modelo, constará de una serie de parámetros (coeficientes), \\(\\{\\beta_0, \\beta_1, \\beta_2, \\dots, \\beta_p\\}\\), cuyo valor podemos ajustar a partir de \\(p\\) variables de entrada proporcionadas al sistema, \\(\\{x_1, x_2, \\dots, x_p\\}\\) para predecir el valor esperado de la variable de salida (respuesta), \\(y\\). En su caso más sencillo, la regresión lineal plantea la ecuación:\n\\[y = \\beta_0 + \\beta_1x_1+\\beta_2x_2+\\dots+\\beta_px_p\\]\nLos valores de los parámetros (coeficientes) \\(\\beta_i\\) se pueden calcular a partir de los datos de entrenamiento. En este caso tan sencillo, todos los parámetros del modelo son calculados a partir de los datos. Sin embargo, en otros casos, además de estos parámetros ajustables a partir de los datos hay que fijar los valores de uno o varios parámetros adicionales, que no pueden deducirse directamente de los datos. Estos son los denominados hiperparámetros, cuyo valor hay que fijar estimándolo de algún modo. Un ejemplo serían los modelos de regresión con regularización, un tipo de penalización que permite minimizar o directamente anular la intervención de las variables de entrada en el resultado de la predicción. Dos variantes comunes en este apartado son la regresión Ridge y Lasso, cuyo parámetro \\(\\alpha\\) controla la “intensidad” del procedimiento de regularización aplicado.",
    "crumbs": [
      "Fundamentos",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Aprendizaje máquina supervisado</span>"
    ]
  },
  {
    "objectID": "02-ml-supervisado.html#el-problema-del-sobreajuste",
    "href": "02-ml-supervisado.html#el-problema-del-sobreajuste",
    "title": "2  Aprendizaje máquina supervisado",
    "section": "2.2 El problema del sobreajuste",
    "text": "2.2 El problema del sobreajuste\nObserva la Figura 2.2, en la que se muestra un pequeño conjunto de datos representado en un diagrama de dispersión en 2-D. Sobre el diagrama de dispersión, tenemos dos posibles modelos candidatos, ajustados a estos datos:\n\nUn modelo lineal simple que, por ejemplo, podríamos haber obtenido mediante un ajuste por mínimos cuadrados.\nUn polinomio de grado \\(n-1\\) (donde \\(n\\) es el número de datos disponibles), que se ajusta perfectamente a todos los datos observados.\n\n\n\n\n\n\n\nFigura 2.2: Ejemplo de sobreajuste de un modelo. La función polinómica (en azul) se ajusta perfectamente a los datos (puntos en negro), pero no generaliza para otros datos nuevos. Fuente: Overfitting, Wikipedia.\n\n\n\nComo podemos apreciar, el modelo lineal simple comete cierto error en su ajuste, ya que algunos puntos quedan ligeramente por encima o por debajo de la recta de regresión propuesta. Sin embargo, el polinomio de grado \\(n-1\\) tiene error nulo, su ajuste es perfecto.\n¿Qué modelo elegiríamos? Podríamos estar tentados de elegir el polinomio, al tener un error menor. Sin embargo, esta sería una elección poco acertada. No es muy creíble que, cualquiera que fuese el proceso que dio origen a estos datos, siga una evolución tan complicada como la marcada por las repetidas (y retorcidas) curvas de ese polinomio. La pregunta clave es no es cuánto error cometemos con los datos de entrenamiento sino cuánto error comete el modelo al tratar de predecir los datos de prueba, que no debe haber visto nunca antes.\nEste problema, clave en el proceso de ajuste/entrenamiento de modelos supervisados, es lo que conocemos como sobreajuste (overfitting). Como norma general, que nunca debemos olvidar, al ajustar un modelo/algoritmo mediante datos tenemos que cuidar que no pierda su capacidad para generalizar, es decir, para predecir con poco margen de error nuevos datos a los que no se haya enfrentando antes.\nEn palabras de George E. P. Box, uno de los más geniales contribuyentes a la práctica estadística del siglo XX y comienzos del XXI, :\n\n“Remember that all models are wrong; the practical question is how wrong do they have to be to not be useful.”\n\n\n\n\n\n\n\nFigura 2.3: George E. P. Box (1919-2013)\n\n\n\nEste principio de generalización debe de completarse con otra prerrogativa, el llamado principio de parsimonia (también conocido como “navaja de Occam”). Este segundo principio nos dicta que debemos favorecer los modelos más sencillos frente a los más complicados, puesto que serán más fáciles de interpretar.\nNo obstante, conviene recordar las palabras de A. Einstein:\n\n“Everything should be made as simple as possible, but not simpler.”\n\nLa traducción de esta advertencia, en términos de nuestro procedimiento para ajustar modelos de aprendizaje automático, es que también debemos evitar el subajuste (underfitting), provocado por un modelo que no ha sido entrenado convenientemente y todavía podría ofrecer menor error de entrenamiento sin incrementar el error de generalización.\n\n\n\n\n\n\nFigura 2.4: Ilustración del efecto del subajuste y el sobreajuste ajustando un modelo de regresión lineal a un conjunto de datos. El modelo parabólico del centro ofrece una buena solución de compromiso. Fuente: https://medium.com/@kiprono_ek/regularization-a-technique-used-to-prevent-over-fitting-886d5b361700.",
    "crumbs": [
      "Fundamentos",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Aprendizaje máquina supervisado</span>"
    ]
  },
  {
    "objectID": "02-ml-supervisado.html#el-balance-sesgo-varianza",
    "href": "02-ml-supervisado.html#el-balance-sesgo-varianza",
    "title": "2  Aprendizaje máquina supervisado",
    "section": "2.3 El balance sesgo-varianza",
    "text": "2.3 El balance sesgo-varianza\nEl “punto de equilibrio” al que hacemos referencia en el apartado anterior está directamente relacionado con dos conceptos esenciales que midel el error cometido por los modelos entrenados en aprendizaje automático (Boehmke & Greenwell, 2019):\n\nSesgo (bias): es la diferencia entre el valor esperado (o promedio) que predice nuestro algoritmo y el valor correcto que estamos intentando predecir. Dicho de otro modo, mide cómo de lejos se quedan los valores predichos por nuestro modelo respecto de los valores reales.\nVarianza (variance): es la “imprecisión” (error) que tienen las predicciones generadas por nuestro modelo para un valor de salida específico. Normalmente, un excesivo error en las predicciones para valores concretos de salida está relacionado con el sobreajuste, al haberse fijado el modelo demasiado en las fluctuaciones de los datos de entrenamiento, evitando así que generalice de forma más precisa para otros casos.\n\nLa Figura 2.5 muestra un esquema de los diferentes errores cometidos durante el proceso de aprendizaje, y dónde se encontraría un posible punto de compromiso, en la zona intermedia del gráfico, en el que se localiza un mínimo error de entrenamiento (para evitar subajuste), impidiendo que crezca de nuevo el error de generalización (lo que evita el sobreajuste).\n\n\n\n\n\n\nFigura 2.5: Ilustración esquemática del sesgo y la varianza en el proceso de entrenamiento de un modelo de aprendizaje máquina supervisado. La zona a la izquierda del punto de compromiso deseado corresponde al subajuste, mientras que la zona a la derecha del punto de compromiso implica caer en sobreajuste. Fuente: https://sebastianraschka.com/pdf/lecture-notes/stat451fs20/08-model-eval-1-intro__slides.pdf.\n\n\n\nLa Figura 2.6 trata de ilustrar el tipo de error cometido en cada caso, comparando nuestras predicciones con lanzamientos sobre una diana (en rojo), asumiendo que la predicción correcta consiste en acertar en el centro de la diana.\n\n\n\n\n\n\nFigura 2.6: Representación conceptual del sesgo y la varianza en predicción de modelos de aprendizaje automático, suponiendo que se tratasen de lanzamientos sobre una diana. Fuente: (Raschka, 2020).\n\n\n\nComo resulta evidente en esta representación, la situación ideal sería la de un modelo que, simultáneamente, consiguiese predicciones con pocas fluctuaciones (varianza pequeña) y cercanas al valor real (mínimo sesgo). Sin embargo, en muchos casos reales no es posible conseguir ambos objetivos al mismo tiempo, y debemos optar por elegir la solución de compromiso que mejor refleje nuestros intereses.",
    "crumbs": [
      "Fundamentos",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Aprendizaje máquina supervisado</span>"
    ]
  },
  {
    "objectID": "02-ml-supervisado.html#sec-cross-validation",
    "href": "02-ml-supervisado.html#sec-cross-validation",
    "title": "2  Aprendizaje máquina supervisado",
    "section": "2.4 Procedimiento de validación cruzada",
    "text": "2.4 Procedimiento de validación cruzada\nEn el contexto que hemos descrito, se sabe que sin un conocimiento previo sobre un problema concreto o el conjunto de datos que vamos a utilizar, es difícil asegurar a priori cuál será el mejor modelo o algoritmo de aprendizaje automático que lo resuelva. Esta limitación se conoce como el teorema no free lunch (Wolpert, 1996).\nPuesto que debemos comparar los posibles modelos candidatos antes de utilizar los datos del conjunto de testing, una posible opción es utilizar los propios datos de entrenamiento para medir el rendimiento de cada modelo y compararlos entre sí. Sin embargo, este procedimiento no es adecuado porque, como ya sabemos, algunos modelos se pueden comportar muy bien con los datos de entrenamiento, pero generalizan mal con otros datos nuevos.\nOtra opción es la de utilizar un método de validación, en el que parte del conjunto de datos de entrenamiento se utilicen para entrenar el modelo y otra parte, el conjunto de validación (holdout set) se guarde para validar el modelo, sin usar aún el conjunto de prueba final, que queda al margen de este proceso.\nSin embargo, usar un solo conjunto de validación puede dar como resultado malas estimaciones (se pueden consultar las referencias ofrecidas en apartado 2.4 de (Boehmke & Greenwell, 2019)). La mejor opción es usar un método de remuestreo (resampling). La idea es repetir el ajuste del modelo sobre distintas fracciones de los datos de entrenamiento y evaluar su rendimiento sobre otras partes. Los dos métodos más habituales que se aplican en estos casos son:\n\nValidación cruzada con \\(k\\) conjuntos (k-fold cross validation).\nBootstrapping, basada en la conocida técnica bootstrap de remuestreo: una muestra aleatoria con reemplazo de los datos originales (Efron & Tibshirani, 1993).\n\n\n\n\n\n\n\nAdvertencia\n\n\n\nEs importante saber que en algunos textos y referencias sobre métodos de validación, se denomina indistintamente a los sucesivos conjuntos de datos de validación dentro de grupo de datos de entrenamiento como “prueba”, “test”, “validación”, “evaluación”, etc. Esto genera gran confusión, puesto que es difícil distinguir cuándo se referieren a estos conjuntos y cuándo están hablando de los datos de evaluación finales, que permanecen al margen durante todo el proceso hasta que tenemos que evaluar el modelo final seleccionado.\nEn consecuencia, lo importante es tener claro el esquema mental del proceso que estamos siguiendo, para interpretar adecuadamente la denominación que utilicemos en cada referencia.\n\n\n\n2.4.1 Validación cruzada con k conjuntos\nLa Figura 2.7 muestra en un esquema cómo se implementa este método de validación.\n\n\n\n\n\n\nFigura 2.7: Esquema que explica el procedimiento de validación cruzada con k conjuntos. Fuente: Sec. 2.4.1 de (Boehmke & Greenwell, 2019).\n\n\n\n\nEl conjunto global de datos de entrenamiento se divide en \\(k\\) trozos o subconjuntos.\nEn cada iteración, uno de los trozos se reserva para validar los modelos y los otros \\(k-1\\) trozos se usan para entrenar los modelos.\nPor último, para cada modelo se combinan (calculando el promedio o de otras formas) las evaluaciones individuales en cada iteración para ofrecer un resultado de evaluación global, que se compara con el resto de modelos para seleccionar el más adecuado.\n\nLo más habitual es usar valores \\(k=5\\) o \\(k=10\\) (Boehmke & Greenwell, 2019). Cuanto mayor sea \\(k\\) menor será la diferencia entre el rendimiento real y nuestra estimación, pero también será mayor el coste computacional para completar el procedimiento.\n\n2.4.1.1 k-fold cross validation con R\nLa mayoría de paquetes orientados al entrenamiento y evaluación de modelos de aprendizaje automático en R ya incluyen argumentos para indicar el tipo de validación cruzada que queremos realizar.\nPor ejemplo, la función h2o::h2o.glm() para ajuste de modelos GLM (véase Sección 3.3), incluye el argumento de entrada nfolds para controlar cuántos grupos usaremos en el procedimiento de validación cruzada. Si nfolds = 0 no se aplica validación cruzada.\n\nh2o.cv &lt;- h2o.glm(\n  x = x, \n  y = y, \n  training_frame = ames.h2o,\n  nfolds = 10  # realiza 10-fold CV\n)\n\nEl paquete rsample contiene la función rsample::vfold_cv() que devuelve un data frame anidado. Cada elemento en splits es una lista que contiene el data frame de entrenamiento en esa iteración y los IDs de las observaciones que se usarán para entrenar vs. validar el modelo.\n\nvfold_cv(ames, v = 10) # prepara 10-fold CV\n\n\n\n\n2.4.2 Boostrapping\nLa Figura 2.8 muestra un esquema de cómo se realiza el proceso de muestreo con reemplazo o bootstrapping.\n\n\n\n\n\n\nFigura 2.8: Esquema que explica el procedimiento de bootstrapping. Fuente: Sec. 2.4.2 de (Boehmke & Greenwell, 2019).\n\n\n\nEn promedio, un 63,21% de las muestras originales son seleccionadas en cada muestra boostrap, mientras que las restantes que no han sido seleccionadas se marcan como out-of-bag (OOB). En este procedimiento, podemos entrenar un modelo con la muestra boostrap y validarlo con las muestras OOB. Por ejemplo este es el método empleado por Random Forests.\nEste procedimiento tiende a reducir la varianza de la estimación, pero en conjuntos de datos no muy grandes (&lt; 1.000 muestras) puede incrementar el sesgo de la estimación de rendimiento. En la mayoría de conjuntos de datos actuales, con tamaños relativamente grandes, esto no suele constituir un problema.\n\nMuestras con bootstrapping en R\nLa función rsample::bootstraps() permite generar los conjuntos que necesitemos mediante bootstrapping.\n\nbootstraps(ames, times = 10)\n\n\n\n\n2.4.3 Otros métodos de validación cruzada\nExisten muchos otros métodos de validación cruzada que contemplan casos especiales o se aplican sobre datos con características particulares.\n\nValidación cruzada anidada: Método alternativo de validación que permite estimar los hiperparámetros y comparar al mismo tiempo modelos introduciendo menor sesgo (cf. Sec. 4.14 (Raschka, 2020)).\nLOOCV: es un caso extremo de validación cruzada con \\(k\\) conjuntos, en el que \\(k=n\\) (Kuhn & Johnson, 2013).\nValidación cruzada para series temporales: se aplica cuando tenemos datos con depencias temporales estrictas. Puedes consultar la sec. 5.10 de (Hyndman & Athanasopoulos, 2021) para aprender más sobre esta técnica.\n\nEn el extenso artículo sobre evaluación de modelos de aprendizaje automático de S. Raschka (Raschka, 2020) se pueden consultar más propiedades y consejos prácticos sobre métodos de validación.\n\n\n\n\nBoehmke, B., & Greenwell, B. (2019). Hands-On Machine Learning with R (1.ª ed.). Chapman; Hall/CRC. https://doi.org/10.1201/9780367816377\n\n\nEfron, B., & Tibshirani, R. J. (1993). Bootstrap Methods and Their Application. Chapman; Hall/CRC.\n\n\nGrigorev, A. (2021). Machine Learning Bookcamp (1.ª ed.). Manning Publications Co. https://www.manning.com/books/machine-learning-bookcamp\n\n\nHyndman, R. J., & Athanasopoulos, G. (2021). Forecasting: Principles and Practice (3.ª ed.). OTexts. https://otexts.com/fpp3/\n\n\nKuhn, M., & Johnson, K. (2013). Applied Predictive Modeling. Springer New York. https://books.google.es/books?id=xYRDAAAAQBAJ\n\n\nRaschka, S. (2020). Model Evaluation, Model Selection, and Algorithm Selection in Machine Learning. https://arxiv.org/abs/1811.12808\n\n\nWolpert, D. H. (1996). The Lack of A Priori Distinctions Between Learning Algorithms. Neural Computation, 8(7), 1341-1390. https://doi.org/10.1162/neco.1996.8.7.1341",
    "crumbs": [
      "Fundamentos",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Aprendizaje máquina supervisado</span>"
    ]
  },
  {
    "objectID": "03-algoritmos-modelos.html",
    "href": "03-algoritmos-modelos.html",
    "title": "3  Algoritmos y modelos",
    "section": "",
    "text": "3.1 Datasets\nEn este apartado, vamos a presentar algunos modelos de aprendizaje máquina supervisado que, posteriormente, mostraremos cómo se pueden entrenar y evaluar con R. Necesitaremos los siguientes paquetes.\nAdemás, será necesario tener instalados varios paquetes más que caret emplea internamente para ajustar cada tipo de algoritmo o modelo solicitado.\nAntes de comenzar, es importante hacer una distinción entre dos tipos de modelos o algoritmos (James, 2021):\nEn este capítulo vamos a utilizar el paquete caret en modo de ejecución paralela, para aprovechar las capacidades multi-núcleo de la mayoría de computadores modernos.\nDataset: Sonar: El objetivo es discriminar entre dos tipos de blancos detectados con un sistema sónar (Gorman & Sejnowski, 1988):\nEstá incluido en el paquete mlbench Blake & Merz (1998).",
    "crumbs": [
      "Fundamentos",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Algoritmos y modelos</span>"
    ]
  },
  {
    "objectID": "03-algoritmos-modelos.html#datasets",
    "href": "03-algoritmos-modelos.html#datasets",
    "title": "3  Algoritmos y modelos",
    "section": "",
    "text": "Cilindro metálico.\nRoca aproximadamente cilíndrica.\n\n\n\n\n\nTabla 3.2: Descripción del dataset mlbench::Sonar.\n\n\n\n\n\nDescripción\nValor\n\n\n\n\nTipo de problema\nClasificación binaria (supervisada)\n\n\nRespuesta\nClass; etiquetas: M o R\n\n\nVariables (inputs)\n60; rango \\([0.0, 1.0]\\)\n\n\nObservaciones\n208\n\n\nObjetivo\nDistinguir tipo de blanco sónar\n\n\nMás información\n?mlbench::Sonar\n\n\n\n\n\n\n\n3.1.1 Particionado de datos\nDataset: mlbench::Sonar.\nLa preparación esta documentada en el paquete caret.\n\ndata(Sonar)\nSonarData = Sonar[,c(1,11,17,23,28,34,40,45,56,61)]\n\nset.seed(107)\ninTrain &lt;- createDataPartition(\n  y = SonarData$Class,  # se necesitan los datos de la salida\n  p = .75,              # 75% datos para entrenamiento\n  list = FALSE\n)\n## Formato de resultados\n\n## La salidad es un conjunto de enteros que representan los\n## índices de las filas en el dataset original escogidos para\n## entrar en el training set.\nstr(inTrain)\n\n int [1:157, 1] 1 2 3 4 5 7 10 11 12 13 ...\n - attr(*, \"dimnames\")=List of 2\n  ..$ : NULL\n  ..$ : chr \"Resample1\"\n\ntraining &lt;- SonarData[ inTrain,]\ntesting  &lt;- SonarData[-inTrain,]\n\nnrow(training)\n\n[1] 157\n\nnrow(testing)\n\n[1] 51\n\n\nSólo queda configurar es sistema de evaluación cruzada para medir el rendimiento de los modelos antes de enfrentarse al conjunto final de prueba, así como para ajustar los hiperparámetros que sean necesarios. En todos los ejemplos aplicaremos validación cruzada con 5 grupos.\n\nconfig_control &lt;- trainControl(\n    method = 'cv',                   # k-fold cross-validation\n    # method = 'repeatedcv',         # k-fold cross-validation repetida (variante)\n    number = 5,                      # número de grupos o iteraciones de remuestreo\n    # repeats = 10,                  # número de conjuntos completos de grupos a computar\n    savePredictions = 'final',       # guardar las predicciones para ajuste óptimo de hiperparámetros\n    classProbs = TRUE,               # se devuelven las probabilidades de clase\n    summaryFunction=twoClassSummary  # tipo de función para resumir resultados (según el tipo de problema)\n)",
    "crumbs": [
      "Fundamentos",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Algoritmos y modelos</span>"
    ]
  },
  {
    "objectID": "03-algoritmos-modelos.html#k-vecinos-más-cercanos-k-nn",
    "href": "03-algoritmos-modelos.html#k-vecinos-más-cercanos-k-nn",
    "title": "3  Algoritmos y modelos",
    "section": "3.2 K vecinos más cercanos (K-NN)",
    "text": "3.2 K vecinos más cercanos (K-NN)\nSe trata de un modelo que puede realizar funciones de regresión o clasificación. En el caso de la regresión, fijado un valor para \\(K\\) y un punto de predicción \\(x_0\\), se identifican las \\(K\\) observaciones del conjunto de entrenamiento más cercanas a \\(x_0\\), denotadas por \\(\\mathcal{N}_0\\). Entonces, se estima \\(f(x_0)\\) como el valor promedio de todas las respuestas contenidas en \\(\\mathcal{N}_0\\), es decir:\n\\[\n\\hat{f}(x_0) = \\frac{1}{K}\\sum_{x_i \\in \\mathcal{N}_0} \\,y_i.\n\\tag{3.1}\\]\n\n\n\n\n\n\nFigura 3.1: Valores estimados para una regresión KNN en un conjunto de datos en dos dimensiones con 64 observaciones. Izq.: K=1. Dcha.: K=9. Fuente: Fig. 3.16 (James, 2021).\n\n\n\nEste modelo se basa en la premisa de que las observaciones similares se encontrarán próximas entre sí dentro del espacio de representación de los datos. Sin embargo, no siempre es fácil encontrar espacios de representación que describan nuestro problema de forma adecuada para que se cumpla esta premisa. En todo caso, este modelo ha demostrado ser muy útil en gran variedad de problemas.\nEn la Sec. 8.2 de (Boehmke & Greenwell, 2019), se discuten algunas medidas de disimilaridad que se pueden computar entre pares de observaciones. Algunas funciones de distancia son la familia de distancias Minkowski, incluyendo Manhattan (L1), Euclídea (L2) o Chebyshev (Linf); la distancia de Haversine (para datos geolocalizados sobre la superficie terrestre); la similaridad del coseno (para documentos o datos textuales) o la distancia de Jaccard.\nEl modelo KNN para clasificación es muy directo. Sea \\(K\\) un número entero positivo y \\(x_0\\) una observación que deseamos clasificar. Primero, el algoritmo identifica las \\(K\\) observaciones más cercanas a \\(x_0\\), que denotamos por \\(\\mathcal{N}_0\\). Entonces, se estima la probabilidad condicional para la clase \\(j\\) como la fracción de puntos pertenecientes a \\(\\mathcal{N}_0\\) cuyos valores de respuesta son la clase \\(j\\). En otras palabras, se asigna la etiqueta de salida de la mayorí ade vecinos cercanos como la clase más probable para la salida a predecir para el punto \\(x_0\\):\n\\[\nP(Y=j \\mid X=x_0)= \\frac{1}{K}\\sum_{i \\in \\mathcal{N}_0} \\,I(y_i = j).\n\\tag{3.2}\\]\n\n3.2.1 Ajuste de un modelo de clasificación KNN\nEn el siguiente ejemplo se muestra cómo ajustar un modelo KNN para clasificación con el dataset mlbench::Sonar. La métrica que emplearemos para ajustar el hiperparámetro \\(K\\) es la curva ROC, presentada en la Sección 4.2.2.\n\nmodel_knn = train(Class ~ ., data=training, method='knn',\n                       metric=\"ROC\", trControl = config_control)\nmodel_knn\n\nk-Nearest Neighbors \n\n157 samples\n  9 predictor\n  2 classes: 'M', 'R' \n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 126, 126, 126, 125, 125 \nResampling results across tuning parameters:\n\n  k  ROC        Sens       Spec     \n  5  0.8092927  0.8213235  0.6733333\n  7  0.7894853  0.8213235  0.5895238\n  9  0.7808088  0.8095588  0.6285714\n\nROC was used to select the optimal model using the largest value.\nThe final value used for the model was k = 5.",
    "crumbs": [
      "Fundamentos",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Algoritmos y modelos</span>"
    ]
  },
  {
    "objectID": "03-algoritmos-modelos.html#sec-linear-models",
    "href": "03-algoritmos-modelos.html#sec-linear-models",
    "title": "3  Algoritmos y modelos",
    "section": "3.3 Modelos lineales",
    "text": "3.3 Modelos lineales\nLos modelos lineales asumen que existe una relación lineal entre las variables de entrada (inputs, features) y la salida que se desea predecir. Cuidado, porque esto no implica necesariamente que la forma de la función ajustada sea una línea recta, una confusión bastante común que suele asaltar a muchas personas.\nEl dos ejemplos muy básicos de este tipo de modelos son la regresión lineal simple y la regresión lineal múltiple, presentados en detalle en el Capítulo 4 de (Boehmke & Greenwell, 2019), así como en el Capítulo 3 de (James, 2021). Sin embargo, un polinomio de grado (parábola) sigue siendo un modelo lineal ya que, aunque su forma son sea una línea recta, la ecuación se sigue expresando como una combinación lineal de varias componentes.\nLos Modelos Lineales Generalizados (GLM por sus siglas en inglés) extienden el modelo lineal original para responder a muchos tipos de problemas: variables respuesta que no siguen una distribución normal, relaciones no lineales (no recta) entre las entradas y la variable de salida, o salidas de tipo categórico (binarias, multinomiales, ordenadas, etc.).\nEste tipo de modelos consta de tres elementos (Agresti, 2015):\n\nFunción de enlace: define una conexión entre el valor esperado (media) de la variable respuesta con la combinación lineal de las variables de entrada, lo que permite definir modelos que no sigan la forma de una recta (más flexibles), capturando relaciones más complejas.\nDistribución de probabilidad (componente aleatoria): describe el “ruido” o variabilidad de los datos alrededor del valor promedio que se predice. Distribuciones habituales son la Normal, Binomial o Poisson.\nEstructura lineal: todas las variables de entrada se relacionan entre sí mediante una combinación lineal, ponderadas por coeficientes que se ajustan a partir de los datos de entrenamiento. Sin embargo, la estructura generalizada del modelo permite asumir formas no lineales, así como incorporar variables de entrada categóricas, mediante la codificación adecuada.\n\nLa Tabla 3.3 presenta algunos modelos GLM frecuentemente empleados.\n\n\n\nTabla 3.3: Algunos modelos GLM y sus elementos constitutivos asociados.\n\n\n\n\n\n\n\n\n\n\nModelo\nComp. aleatoria\nCol3\n\n\n\n\nRegresión lineal\nNormal\nCuantitativas o cualitativas\n\n\nRegresión logística\nBinomial\nCuantitativas o cualitativas\n\n\nLogLinear\nPoisson\nCualitativas\n\n\nReg. de Poisson\nPoisson\nCuantitativas o cualitativas\n\n\n\n\n\n\n\n3.3.1 Ajuste de un modelo de clasificación LR\nSeguidamente, se presenta el proceso de ajuste y evaluación de un modelo de clasificación mediante regresión logística (LR). Este modelo estima una probabilidad a la salida y debemos establecer un umbral o threshold para dicha probabilidad estimada, a partir de cual (para valores superiores o iguales) asignamos una de las dos posibles etiquetas de salida (por ejemplo, 1). Para una probabilidad estimada menor que el umbral asignamos la otra posible etiqueta de salida (por ejemplo, 0).\n\n# Regresión logística\nmodel_glm &lt;- train(\n  Class ~ .,\n  data = training,\n  method = \"glm\",\n  metric = \"ROC\",\n  trControl = config_control\n)\nmodel_glm\n\nGeneralized Linear Model \n\n157 samples\n  9 predictor\n  2 classes: 'M', 'R' \n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 125, 126, 126, 125, 126 \nResampling results:\n\n  ROC        Sens       Spec     \n  0.8034314  0.7610294  0.7828571\n\n# Recuperamos los coeficientes del modelo final ajustado\nsummary(model_glm$finalModel)\n\n\nCall:\nNULL\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   2.4778     1.5031   1.648 0.099268 .  \nV1          -17.1009    12.1553  -1.407 0.159466    \nV11          -8.2378     2.3484  -3.508 0.000452 ***\nV17           2.3566     1.0500   2.244 0.024808 *  \nV23          -2.3698     1.0091  -2.348 0.018859 *  \nV28           0.4797     0.9750   0.492 0.622699    \nV34           2.0068     1.0909   1.840 0.065818 .  \nV40           3.3099     1.5339   2.158 0.030933 *  \nV45          -8.4562     2.1913  -3.859 0.000114 ***\nV56         -42.1597    42.1564  -1.000 0.317273    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 216.88  on 156  degrees of freedom\nResidual deviance: 144.85  on 147  degrees of freedom\nAIC: 164.85\n\nNumber of Fisher Scoring iterations: 5\n\n\n\n\n3.3.2 Regularización en modelos lineales\nEn los conjuntos de datos actuales, normalmente de gran tamaño, existe el riesgo de que los modelos lineales tiendan al sobreajuste de los datos de entrenamiento, incrementando nuestro error de generalización. Una estrategia muy útil para restringir este efecto pernicioso es la regularización del modelo, que consiste en aplicar una serie de “penalizaciones” a los coeficientes estimados para reducir la varianza (dentro del compromiso varianza-sesgo) y así mantener a raya el problema del sobreajuste.\nTres opciones muy comunes de regularización son:\n\nPenalización Ridge: modifica la función objetivo de ajuste del modelo con un término controlado por un hiperparámetro \\(\\lambda\\). Cuanto más crece el valor asignado a \\(\\lambda\\) más se fuerza a que los coeficientes del modelo se vayan haciendo cada vez más pequeños, aunque sin llegar a anularse. Véase el Apartado 6.2.1 de (Boehmke & Greenwell, 2019).\nPenalización Lasso: altera la función objetivo para ajustar el modelo con un térmio también controlado por un hiperparámetro \\(\\lambda\\). Sin embargo, al contrario que en el caso anterior, cuando \\(\\lambda\\) crece se van anulando progresivamente más coeficientes de la función de predicción, lo que constituye un método más drástico de selección de variables y simplificación de nuestro modelo. Véase el Apartado 6.2.2 de (Boehmke & Greenwell, 2019).\nElastic net: es una solución intermedia entre los dos casos anteriores, introduciendo simultáneamente ambos tipos de penalización (Ridge y Lasso), cada uno de ellos controlado por un hiperparámetro de penalización, \\(\\lambda_1\\) y \\(\\lambda_2\\), respectivamente. Véase el Apartado 6.2.3 de (Boehmke & Greenwell, 2019).\n\nEn la Figura 3.2 se representan los errores (todos los puntos sobre una elipse tienen el mismo valor de RSS) y las funciones de restricción impuestas en el caso de la penalización Lasso y Ridge. Podemos observar como en el caso del Lasso las restricciones tienen aristas, lo que hace que la intersección entre el contorno y la región de restricción se produzca sobre el eje. Cuando esto ocurre, los coeficientes de la función de regresión se anulan. Sin embargo, en Ridge el punto de intersección no llega a tocar el eje, por lo que los coeficientes no llegan a anularse.\n\n\n\n\n\n\nFigura 3.2: Gráficos de contorno para los errores en las estimaciones y funciones de restricción para Lasso (izq.) y Ridge (dcha.). Fuente: Fig. 6.7 (James, 2021).",
    "crumbs": [
      "Fundamentos",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Algoritmos y modelos</span>"
    ]
  },
  {
    "objectID": "03-algoritmos-modelos.html#extensiones-del-modelo-lineal",
    "href": "03-algoritmos-modelos.html#extensiones-del-modelo-lineal",
    "title": "3  Algoritmos y modelos",
    "section": "3.4 Extensiones del modelo lineal",
    "text": "3.4 Extensiones del modelo lineal\nAdemás de los modelos polinómicos o los GLM descritos anteriormente, existen más extensiones de los modelos lineales. La aproximación común a muchos de ellos es utilizar funciones polinómicas con formas suaves para ir ajustando tramos de la función de predicción. En consecuencia, la superposición (combinación lineal) de estas funciones suaves dará como resultado un modelo tremendamente flexible que se puede adaptar a problemas muy complicados.\nUn primer ejemplo son los modelos MARS (Multivariate Adaptive Regression Splines), presentados en el Capítulo 7 de (Boehmke & Greenwell, 2019). Otro ejemplo son los modelos GAM (Generalized Additive Models) que se explican en la Sección 7.7 de (James, 2021). En el caso de estos últimos se extiende el modelo lineal manteniendo la combinación aditiva (suma) de los componentes de predicción. Sin embargo, en cada componente se emplea una función lineal \\(f_j(x_{ij})\\), cuya forma debemos estimar a partir de los datos, según la Ecuación 3.3:\n\\[\ny_i = \\beta_0 + \\sum_{j=1}^{p}f_j(x_{ij})+\\epsilon_i\n\\tag{3.3}\\]\nLa Figura 3.3 muestra un ejemplo del resultado de ajustar un modelo GAM utilizando dos variables de entrada cuantitativas y una cualitativa.\n\n\n\n\n\n\nFigura 3.3: Ejemplo de ajuste de un modelo GAM, con dos predictores cuantitativos (izq. y centro) y otro cualitativo (dcha.). Se puede apreciar que las funciones para los predictores cuantiativos son flexibles y suaves. Fuente: Fig. 7.12 (James, 2021).\n\n\n\n\n3.4.1 Ajuste de un modelo GAM para clasificación\nLos modelos GAM pueden aplicarse tanto a problemas de regresión como de clasificación. En el paquete caret, podemos escoger ajustar un modelo de este tipo con el paquete mgcv o bien con el paquete gam.\nEn este caso, podremos ver que la función de enlace (link function) establecida para relacionar la salida con la combinación lineal de los regresores es logit.\n\nmodel_gam_mgcv = train(Class ~ ., data=training, method='gam',\n                  metric=\"ROC\", trControl = config_control)\nmodel_gam_mgcv\n\nGeneralized Additive Model using Splines \n\n157 samples\n  9 predictor\n  2 classes: 'M', 'R' \n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 125, 126, 126, 125, 126 \nResampling results across tuning parameters:\n\n  select  ROC        Sens       Spec     \n  FALSE   0.7728817  0.8095588  0.6171429\n   TRUE   0.7928011  0.7500000  0.7409524\n\nTuning parameter 'method' was held constant at a value of GCV.Cp\nROC was used to select the optimal model using the largest value.\nThe final values used for the model were select = TRUE and method = GCV.Cp.\n\nsummary(model_gam_mgcv$finalModel)\n\n\nFamily: binomial \nLink function: logit \n\nFormula:\n.outcome ~ s(V56) + s(V1) + s(V23) + s(V28) + s(V17) + s(V11) + \n    s(V34) + s(V40) + s(V45)\n\nParametric coefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)  -0.3515     0.2604   -1.35    0.177\n\nApproximate significance of smooth terms:\n          edf Ref.df Chi.sq  p-value    \ns(V56) 0.7416      9  1.815 0.108066    \ns(V1)  0.7876      9  2.435 0.071601 .  \ns(V23) 2.0615      9  7.979 0.009524 ** \ns(V28) 2.4275      9  7.407 0.020306 *  \ns(V17) 1.7828      9  6.784 0.013827 *  \ns(V11) 2.0751      9 16.218 5.11e-05 ***\ns(V34) 3.8758      9 10.983 0.012560 *  \ns(V40) 1.6428      9  3.822 0.074852 .  \ns(V45) 1.0000      9 14.341 0.000108 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =  0.548   Deviance explained = 53.3%\nUBRE = -0.1328  Scale est. = 1         n = 157\n\n\n\nmodel_gam = train(Class ~ ., data=training, method='gamSpline',\n                  metric=\"ROC\", trControl = config_control)\nmodel_gam\n\nGeneralized Additive Model using Splines \n\n157 samples\n  9 predictor\n  2 classes: 'M', 'R' \n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 126, 125, 126, 125, 126 \nResampling results across tuning parameters:\n\n  df  ROC        Sens       Spec     \n  1   0.8047129  0.7639706  0.7295238\n  2   0.8616597  0.8110294  0.7961905\n  3   0.8607353  0.7992647  0.7676190\n\nROC was used to select the optimal model using the largest value.\nThe final value used for the model was df = 2.\n\ncoef(model_gam$finalModel)\n\n   (Intercept) s(V56, df = 2)  s(V1, df = 2) s(V23, df = 2) s(V28, df = 2) \n      2.174201     -53.809688     -17.909083      -2.124357       0.518643 \ns(V17, df = 2) s(V11, df = 2) s(V34, df = 2) s(V40, df = 2) s(V45, df = 2) \n      2.457338      -6.963477       2.165397       3.161222      -9.148652",
    "crumbs": [
      "Fundamentos",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Algoritmos y modelos</span>"
    ]
  },
  {
    "objectID": "03-algoritmos-modelos.html#máquinas-de-vector-soporte-svm",
    "href": "03-algoritmos-modelos.html#máquinas-de-vector-soporte-svm",
    "title": "3  Algoritmos y modelos",
    "section": "3.5 Máquinas de vector soporte (SVM)",
    "text": "3.5 Máquinas de vector soporte (SVM)\nLas SVM son ejemplos de modelos de clasificación (salida cualitativa), en los que el objetivo es encontrar un hiperplano que separe de la mejor forma posible los elementos pertenecientes a dos grupos (asumiendo que la salida es una variable binaria).\nLa Figura 3.4 muestra cómo se identifican los llamados “vectores soporte” para definir la frontera de separación entre los puntos pertenecientes a los dos grupos de la variable de salida, para el caso de un clasficador de margen rígido (es decir, que no admite que puntos de uno de los grupos desborden la frontera con el otro grupo).\n\n\n\n\n\n\nFigura 3.4: Ilustración del método para encontrar los vectores soporte, que definen la frontera de separación entre los dos grupos de la variable de salida en un modelo SVM. Se asume un clasficador de margen rígido (HMC). Fuente: Fig. 14.3 (Boehmke & Greenwell, 2019).\n\n\n\nLo normal es utilizar una versión más flexible de este algoritmo, que tolera que existan puntos mal clasificados (i.e. en el lado incorrecto de la frontera), dentro de un cierto margen de error. No obstante, la verdadera clave de estos modelos es que aplican el llamado kernel trick, una argucia matemática que nos permite representar los datos en un espacio alternativo en el que la separación entre las fronteras sea calculable. Matemáticamente, entender esta herramienta implica el manejo de funciones núcleo (kernel functions) y comprender los Espacios de Hilbert de Núcleo Reproductor (RKHS). Puedes consultar estos apuntes de un profesor de UCL para una introducción a estos temas.\nEn la práctica, el hecho de que no podamos apreciar directamente los detalles del espacio alternativo en el que se están representando los datos hace que a estos modelos se les considere en cierta medida como de “caja negra” (black-box models). En consecuencia, otras de las limitaciones es que no resulta nada evidente explicar el papel que juega cada una de las variables en la identificación de la frontera de separación entre clases.\n\n\n\n\n\n\nFigura 3.5: Fronteras de separación entre dos grupos de datos que se organizan en forma de espiral. Izq.: frontera de clasificación determinada mediante un algoritmo RF. Dcha.: frontera de clasificación identificada mediante un algoritmo SVM que usa una función núcleo de base radial (radial basis kernel). Fuente: Fig. 14.7 (Boehmke & Greenwell, 2019).\n\n\n\n\n3.5.1 Ajuste de un modelo SVM (RBF)\nMostramos un ejemplo de cómo entrenar un modelo SVM, utilizando como función kernel la función de base radial (Radial Basis Function). Los hiperparámetros que controlan la complejidad del modelo son sigma y C.\n\nmodel_svmRadial = train(Class ~ ., data=training, method='svmRadial',\n                        metric=\"ROC\", tuneLength=15, trControl = config_control)\nmodel_svmRadial\n\nSupport Vector Machines with Radial Basis Function Kernel \n\n157 samples\n  9 predictor\n  2 classes: 'M', 'R' \n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 125, 125, 127, 125, 126 \nResampling results across tuning parameters:\n\n  C        ROC        Sens       Spec     \n     0.25  0.8440616  0.8441176  0.7104762\n     0.50  0.8624300  0.8316176  0.7095238\n     1.00  0.8698669  0.8323529  0.7371429\n     2.00  0.8789391  0.8198529  0.7514286\n     4.00  0.8760784  0.8198529  0.7390476\n     8.00  0.8634139  0.8198529  0.7657143\n    16.00  0.8351225  0.8198529  0.7257143\n    32.00  0.8435609  0.8080882  0.7123810\n    64.00  0.8480917  0.7705882  0.7266667\n   128.00  0.8480917  0.8308824  0.6990476\n   256.00  0.8480917  0.7955882  0.7123810\n   512.00  0.8480917  0.7955882  0.6990476\n  1024.00  0.8480917  0.7955882  0.6990476\n  2048.00  0.8480917  0.7838235  0.6980952\n  4096.00  0.8480917  0.8191176  0.6990476\n\nTuning parameter 'sigma' was held constant at a value of 0.07543981\nROC was used to select the optimal model using the largest value.\nThe final values used for the model were sigma = 0.07543981 and C = 2.",
    "crumbs": [
      "Fundamentos",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Algoritmos y modelos</span>"
    ]
  },
  {
    "objectID": "03-algoritmos-modelos.html#modelos-probabilísticos-naïve-bayes",
    "href": "03-algoritmos-modelos.html#modelos-probabilísticos-naïve-bayes",
    "title": "3  Algoritmos y modelos",
    "section": "3.6 Modelos probabilísticos: Naïve Bayes",
    "text": "3.6 Modelos probabilísticos: Naïve Bayes\nUn ejemplo de modelos de clasificación probabilísticos es el llamado Naïve Bayes (Bayes ingenuo), cuyo funcionamiento se basa en una aplicación directa del Teorema de Bayes. Se emplean con frecuencia en problemas de clasificación con datos textuales, cuando tenemos muchas variables de entrada o bien cuando el rango de valores de las variables de entrada es muy amplio.\nRecordemos que el Teorema de Bayes viene dado por:\n\\[\nP(B \\mid A) = \\frac{P(B)P(A \\mid B)}{P(A)}\n\\tag{3.4}\\]\nSi consideramos que las variables predictoras son independientes entre sí, entonces tenemos que la fórmula de predicción es:\n\\[\nP(Y_k \\mid X_1, \\dots, X_p) = \\frac{P(Y_k)\\prod_{j=1}^p P(X_j \\mid Y_k)}{P(X_1, X_2, \\dots, X_p)}\n\\tag{3.5}\\]\nPuesto que el denominador es una constante, nos centramos en calcular el valor del numerador para poder comparar las probabilidades condicionadas a los valores de las variables de entrada. La clase predicha maximiza la expresión:\n\\[\n\\underset{x}{\\arg\\max} \\left\\{ P(Y_k) \\prod_{j=1}^p P(X_j \\mid Y_k)\\right\\}.\n\\]",
    "crumbs": [
      "Fundamentos",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Algoritmos y modelos</span>"
    ]
  },
  {
    "objectID": "03-algoritmos-modelos.html#redes-neuronales-y-aprendizaje-profundo",
    "href": "03-algoritmos-modelos.html#redes-neuronales-y-aprendizaje-profundo",
    "title": "3  Algoritmos y modelos",
    "section": "3.7 Redes neuronales y aprendizaje profundo",
    "text": "3.7 Redes neuronales y aprendizaje profundo\nMuchos modelos de aprendizaje automático solamente incorporan una o dos capas de transformación de datos para aprender la representación de los datos de salida. Estos modelos se denominan superficiales (shallow models). En contraste, los modelos profundos (deep models) siguen una aproximación multicapa para aprender las representaciones de los datos. El caso más habitual es el de usar múltiples capas de redes neuronales. El Capítulo 13 de (Boehmke & Greenwell, 2019) y el Capítulo 10 de (James, 2021) proporcionan dos buenas introducciones a este tipo de modelos.\nLa Figura 3.6 muestra el diseño de una red neuronal con dos capas ocultas que podría utilizarse para predecir los 10 posibles valores de salida del dataset MNIST, con imágenes de cifras manuscritas.\n\n\n\n\n\n\nFigura 3.6: Esquema de un modelo de clasificación basado en una red neuronal con dos capas ocultas y varias posibles salidas, que se puede aplicar al conjunto de datos MNIST de cifras manuscritas. Fuente: Fig. 10.4 (James, 2021).\n\n\n\nLa clave para que una red neuronal de aprendizaje profundo se autoajuste en base a los datos de entrenamiento es un proceso denominado retropropagación (backpropagation). Este proceso se explica, por ejemplo, en la Sec. 13.5 de (Boehmke & Greenwell, 2019), así como en la Sec. 10.7.1 de (James, 2021), entre otras muchas fuentes.\n\n\n\n\n\n\nTipVideotutorial sobre aprendizaje profundo\n\n\n\nEl sitio web https://www.3blue1brown.com/ contiene un extenso catálogo de videotutoriales y sesiones formativas sobrfe muchos temas de interés, como redes neuronales, álgebra o cálculo.\nEl vídeo What is backpropagation really doing? es una de las mejores explicaciones intuitivas para entender mejor el papel de la retropropagación en el entrenamiento de redes neuronales para aprendizaje profundo.",
    "crumbs": [
      "Fundamentos",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Algoritmos y modelos</span>"
    ]
  },
  {
    "objectID": "03-algoritmos-modelos.html#ensamblado-de-modelos",
    "href": "03-algoritmos-modelos.html#ensamblado-de-modelos",
    "title": "3  Algoritmos y modelos",
    "section": "3.8 Ensamblado de modelos",
    "text": "3.8 Ensamblado de modelos\nEl ensamblado de modelos es una aproximación para resolver el problema del aprendizaje máquina que consiste en combinar la salida de múltiples modelos individuales para dar una predicción final que mejora el rendimiento que podríamos alcanzar con un solo modelo. La referencia más completa para entender bien esta estrategia de aprendizaje máquina es (Kuncheva, 2014).\nAlgunos de los modelos más conocidos son:\n\nBagging (Bootstrap AGGregatING): consiste en el ensamblado de modelos de clasificación entrenados sobre réplicas bootstrap* de los datos de entrenamiento originales. La salida de los clasificadores individuales se combina mediante el voto de pluralidad. Utilizar el voto mayoritario para tomar la decisión garantiza que vamos a obtener un resultado que mejora el de cada modelo individual. El Capítulo 10 de (Boehmke & Greenwell, 2019) muestra en detalle ejemplos de esta técnica.\nRandom Forests (bosques aleatorios): fueron propuestos por el insigne Leo Breiman en 2001 (Breiman, 2001). Es una modificación de la estrategia de bagging aplicada a modelos de árboles de decisión, que emplea una amplia colección de árboles decorrelados entre sí para mejorar la eficiencia de predicción de la variable de salida. Además de tomar muestras bootstrap de los datos de entrenamiento, este método realiza selecciones aleatorias de las variables de entrada en cada nodo del árbol, tras lo cual se selecciona de entre las características tomadas la azar la mejor para dividir los caminos desde ese nodo. Es uno de los métodos más populares hoy en día, puesto que ofrece un buen rendimiento con un coste computacional contenido y con relativamente poco esfuerzo de ajuste de hiperparámetros. El Capítulo 11 de (Boehmke & Greenwell, 2019) muestra el trabajo con este tipo de algoritmos.\nBoosting: propone la construcción de un ensamblado de árboles poco profundos, secuencialmente, en el que cada árbol que se añade mejora al anterior. Aunque cada árbol poco profundo es una herramienta de aprendizaje débil, pueden ser “potenciados” (boosted) de este modo para crear un comité de modelos que ofrece un rendimiento muy bueno. Uno de los primeros métodos propuestos con esta estrategia fue AdaBoost. Una de las variantes más populares actualmente es XGBoost (Extreme Gradient Boosting) (véase Sec. 12.5 de (Boehmke & Greenwell, 2019)), que incluye hiperparámetros para controlar términos de penalización del modelo que reduzcan su complejidad y prevengan el sobreajuste.\n\n\n3.8.1 Ejemplo de ajuste de un modelo Random Forests (RF)\n\nmodel_rf = train(Class ~ ., data=training, method='rf',\n                 metric=\"ROC\", tuneLength=5, trControl = config_control)\nmodel_rf\n\nRandom Forest \n\n157 samples\n  9 predictor\n  2 classes: 'M', 'R' \n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 125, 127, 125, 126, 125 \nResampling results across tuning parameters:\n\n  mtry  ROC        Sens       Spec     \n  2     0.8592857  0.8330882  0.7942857\n  3     0.8732826  0.7970588  0.7809524\n  5     0.8693452  0.8323529  0.7809524\n  7     0.8782143  0.8323529  0.7942857\n  9     0.8717192  0.8088235  0.7800000\n\nROC was used to select the optimal model using the largest value.\nThe final value used for the model was mtry = 7.\n\n\n\n\n3.8.2 Ejemplo de ajuste de un modelo AdaBoost\n\nmodel_adaboost = train(Class ~ ., data=training, method='AdaBoost.M1',\n                       metric=\"ROC\", tuneLength=2, trControl = config_control)\nmodel_adaboost\n\nAdaBoost.M1 \n\n157 samples\n  9 predictor\n  2 classes: 'M', 'R' \n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 125, 126, 125, 126, 126 \nResampling results across tuning parameters:\n\n  coeflearn  maxdepth  mfinal  ROC        Sens       Spec     \n  Breiman    1          50     0.8149160  0.7735294  0.6571429\n  Breiman    1         100     0.8295518  0.7963235  0.7266667\n  Breiman    2          50     0.8341527  0.7852941  0.7400000\n  Breiman    2         100     0.8396429  0.7970588  0.7257143\n  Freund     1          50     0.8370448  0.7852941  0.7247619\n  Freund     1         100     0.8375700  0.7970588  0.7523810\n  Freund     2          50     0.8490966  0.7500000  0.7114286\n  Freund     2         100     0.8391246  0.7970588  0.6980952\n  Zhu        1          50     0.8489286  0.7375000  0.7390476\n  Zhu        1         100     0.8407633  0.7382353  0.7390476\n  Zhu        2          50     0.8560364  0.8102941  0.7523810\n  Zhu        2         100     0.8264146  0.7742647  0.7228571\n\nROC was used to select the optimal model using the largest value.\nThe final values used for the model were mfinal = 50, maxdepth = 2\n and coeflearn = Zhu.\n\n\n\n\n3.8.3 Ejemplo de ajuste de modelos XGBoost\nXGBoost lineal\n\nmodel_xgbLinear = train(Class ~ ., data=training, method='xgbLinear',\n                        metric=\"ROC\", tuneLength=5, trControl = config_control, verbose=F)\n# Descomentar para ver iteraciones\n# model_xgbLinear\n\n\nstopCluster(cl)",
    "crumbs": [
      "Fundamentos",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Algoritmos y modelos</span>"
    ]
  },
  {
    "objectID": "03-algoritmos-modelos.html#apilado-de-modelos",
    "href": "03-algoritmos-modelos.html#apilado-de-modelos",
    "title": "3  Algoritmos y modelos",
    "section": "3.9 Apilado de modelos",
    "text": "3.9 Apilado de modelos\nPor último, otra posible estrategia de combinación de modelos de aprendizaje individuales es el apilado de modelos (model stacking), que implica el entrenamiento de un nuevo modelo que combina las predicciones de varios modelos de aprendizaje de base. El meta-algoritmo que combina las salidas previas, llamado super learner permite mejorar aún más el rendimiento de los modelos de aprendizaje de base (como RF o XGBoost). En el Capítulo 15 de (Boehmke & Greenwell, 2019) se puede encontrar más información y ejemplos de este tipo de modelos.\n\n\n\n\nAgresti, A. (2015). Foundations of Linear and Generalized Linear Models (1.ª ed.). John Wiley & Sons.\n\n\nBlake, C. L., & Merz, C. J. (1998). UCI Repository of Machine Learning Databases. University of California, Irvine, Department of Information; Computer Sciences.\n\n\nBoehmke, B., & Greenwell, B. (2019). Hands-On Machine Learning with R (1.ª ed.). Chapman; Hall/CRC. https://doi.org/10.1201/9780367816377\n\n\nBreiman, L. (2001). Random forests. Machine learning, 45(1), 5-32.\n\n\nGorman, R. P., & Sejnowski, T. J. (1988). Analysis of Hidden Units in a Layered Network Trained to Classify Sonar Targets. Neural Networks, 1, 75-89. https://doi.org/10.1016/0893-6080(88)90023\n\n\nJames, W., G. (2021). An Introduction to Statistical Learning with Applications in R (2.ª ed.). Springer. https://www.statlearning.com/\n\n\nKuncheva, L. I. (2014). Combining Pattern Classifiers: Methods and Algorithms (2.ª ed.). Wiley-Interscience. https://lucykuncheva.co.uk/Combining_Pattern_Classifiers_Methods_and_Algorithms_2nd_ed_Kuncheva%202014-09-09.pdf\n\n\nLeisch, F., & Dimitriadou, E. (2024). mlbench: Machine Learning Benchmark Problems. https://CRAN.R-project.org/package=mlbench",
    "crumbs": [
      "Fundamentos",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Algoritmos y modelos</span>"
    ]
  },
  {
    "objectID": "04-seleccion-validacion.html",
    "href": "04-seleccion-validacion.html",
    "title": "4  Selección y validación de modelos",
    "section": "",
    "text": "4.1 Consideraciones generales\nHace varias décadas, la evaluación y selección de modelos se efectuaba mediante pruebas estadísticas que medían la bondad de los ajustes y a través de la evaluación de los residuos (errores) del modelo propuesto. Hoy en días, la evaluación se realiza empleando medios mucho más robustos, las conocidas como funciones de pérdida (loss functions).\nUna función de pérdida es una métrica que compara los valores predichos respecto de los valores que realmente se han producido. Su salida se denomina habitualmente error o bien pseudo-residuo. En este contexto, tenemos que tener en cuenta una premisa fundamental:\n\\[\n\\text{DATOS} = \\text{MODELO} + \\text{ERROR}.\n\\]\nExisten muchos tipos de funciones de pérdida para evaluar el rendimiento de modelos predictivos, cada una con sus ventajas e inconvenientes. Es importante recordar que cada función de pérdida se calcula teniendo en cuenta uno o varios tipos de error cometidos por el modelo, dejando otros de lado, por lo que es complicado afirmar mirando una sola de estas funciones que podemos escoger un modelo óptimo. Dependiendo del problema en cuestión, debemos seleccionar primero la función o funciones de pérdida más adecuadas para evaluar las características del modelo que nos interesan. Después, podemos aplicarlas para evaluar los candidatos y seleccionar una opción.",
    "crumbs": [
      "Fundamentos",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Selección y validación de modelos</span>"
    ]
  },
  {
    "objectID": "04-seleccion-validacion.html#consideraciones-generales",
    "href": "04-seleccion-validacion.html#consideraciones-generales",
    "title": "4  Selección y validación de modelos",
    "section": "",
    "text": "Los datos describen la realidad que queremos explicar mediante el modelo, a través de una serie de variables que describen dicho problema.\nEl modelo es una representación simplificada de la realidad, propuesta para intentar describirla de manera más fácilmente comprensible. Cuanto más sencillo sea el modelo más fácil será interpretarlo, pero también puede que cometa más imprecisiones al describir el fenómeno real que estudiamos.\nEl error representa el fallo que cometemos al simplificar la realidad mediante el modelo simplificado.",
    "crumbs": [
      "Fundamentos",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Selección y validación de modelos</span>"
    ]
  },
  {
    "objectID": "04-seleccion-validacion.html#métricas-de-rendimiento",
    "href": "04-seleccion-validacion.html#métricas-de-rendimiento",
    "title": "4  Selección y validación de modelos",
    "section": "4.2 Métricas de rendimiento",
    "text": "4.2 Métricas de rendimiento\n\n4.2.1 Modelos de regresión\n\nError Cuadrático Medio (Mean Squared Error o MSE): el objetivo es minimizar el promedio de la combinación de los errores cuadráticos cometidos en la predicción de cada punto.\n\n\\[\n\\text{MSE} = \\frac{1}{n}\\sum_{i=1}^n (y_i - \\hat{y}_i)^2\n\\tag{4.1}\\]\nHay que tener en cuenta que en modelos de regresión lineal, el divisor de la ecuación Ecuación 4.1 es \\(n-p\\) para proporcionar una mejor estimación (reduciendo el sesgo).\n\nRaíz del Error Cuadrático Medio (Root Mean Squared Error o RMSE): es la raíz cuadrada del MSE. Su fin es dar una estimación del error cometido en las mismas unidades que la variable de salida que buscamos predecir. El objetivo aquí también es minimizar la función de pérdida.\nDesviación: desviación promedio de los residuos, cuando el modelo se estima mediante un método de máxima verosimilitud. Se suele emplear en modelos de clasificación y el objetivo es minimizar su valor.\nError Medio Absoluto (Mean Absolute Error o MAE): en este caso, en lugar de tomar la raíz cuadrada del MSE se calculan las diferencias entre los valores reales y predichos tomando su valor absoluto, lo que contiene la influencia de los errores con valores altos. EL objetivo es minimizar la función de pérdida.\nError Medio Cuadrático Logarítmico (*Root Mean Squared Logarithmic Error** o RMSLE): Es la raíz cuadrada del error medio cuadrático logarítmico. Permite que los errores grandes cometidos sobre valores de salida de magnitud pequeña contribuyan de igual forma a la función de pérdida que los errores grandes cometidos sobre valores de salida de magnitud grande, ya que de otro modo los segundos prevalecerían sobre los primeros. Aquí el objetivo es minimizar igualmente la función de pérdida.\n\n\\[\n\\text{RMSLE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (log(y_i +1) -  log(\\hat{y}_i + 1))^2}.\n\\tag{4.2}\\]\n\n\\(R^2\\) y \\(R_{adj}^2\\): son métricas que representan el porcentaje de la varianza total de los datos que el modelo es capaz de explicar. La versión ajustada intenta tener en cuenta el número de variables predictoras empleadas, penalizando modelos más complejos. De otro modo, usar un mayor número de variables normalmente aumentaría sin más el valor de \\(R^2\\). El objetivo aquí es maximizar su valor. No obstante, es preciso obrar con precaución puesto que, como se indica en (Boehmke & Greenwell, 2019) (Sec. 2.6.2), dos modelos entrenados con datos diferentes que tengan el mismo RMSE, pero en los que el primero tenga menos variabilidad (dispersión) en los valores de la salida del modelo que el segundo tendrá también un \\(R^2\\) menor.\n\n\n\n4.2.2 Modelos de clasificación\nLas métricas de rendimiento de los modelos de clasificación se suelen construir a partir de los valores de la llamada matriz de confusión, que refleja el tipo de aciertos y errores cometidos por el modelo al clasificar, como muesgtra la Figura 4.1.\n\n\n\n\n\n\nFigura 4.1: Representación de los diferentes tipos de aciertos y errores que refleja la matriz de confusión y el nombre que suelen recibir en evaluación de modelos de clasificación. Fuente: Fig. 2.12 (Boehmke & Greenwell, 2019).\n\n\n\n\nAccuracy: se define como (objetivo: maximizar)\n\n\\[\n\\text{ACC} = \\frac{\\text{TP}+\\text{TN}}{\\text{total}}.\n\\tag{4.3}\\]\n\nPrecision: se define como (objetivo: maximizar)\n\n\\[\n\\text{PREC} = \\frac{\\text{TP}}{\\text{TP}+\\text{FP}}.\n\\tag{4.4}\\]\n\nRecall (sensitivity): se define como (objetivo: maximizar)\n\n\\[\n\\text{RECALL}= \\frac{\\text{TP}}{\\text{TP}+\\text{FN}}.\n\\tag{4.5}\\]\n\nSpecificity: se define como (objetivo: maximizar)\n\n\\[\n\\text{SPEC}=\\frac{\\text{TN}}{\\text{TN}+\\text{FP}}.\n\\tag{4.6}\\]\n\nF1-score: se trata de la media armónica entre precisión y recall, dado por\n\n\\[\n\\text{F1} = 2\\;\\frac{\\text{PREC}\\times\\text{RECALL}}{\\text{PREC} + \\text{RECALL}}\n\\tag{4.7}\\]\n\nF-score generalizado: como el anterior, pero añadiendo pesos de ponderación, dado por\n\n\\[\n\\text{F1} = (1+\\beta^2)\\;\\frac{\\text{PREC}\\times\\text{RECALL}}{\\beta^2\\text{PREC} + \\text{RECALL}}\n\\tag{4.8}\\]\nSi \\(\\beta=1\\) tenemos el F1-score. Si \\(\\beta=2\\) otorgamos el doble de peso a la recuperación frente a la precisión y al contrario si \\(\\beta=0.5\\).\n\nAUC (Área Bajo la Curva ROC): dependiendo de los valores otorgados a los hiperparámetros del modelo, podremos mejorar la precisión, la sensitividad o ambas, minimizando los falsos positivos y falsos negativos. Las curvas ROC (Receiver Operating Characteristic) se crearon para represntar gráficamente estos valores en función de cada combinación de los hiperparámetros del modelo, permitiendo una evaluación gráfica más general de su rendimiento. En este caso el Area Under the [ROC] Curve (concretamente, entre la curva ROC y la línea recta diagonal que representa a un clasificador aleatorio) debe ser lo más grande posible. Estos conceptos se muestran en la\n\n\n\n\n\n\n\n\n\n\nFigura 4.2: Representación de varias curvas ROC y su interpretación en evaluación de modelos de clasificación. Fuente: Fig. 2.14 (Boehmke & Greenwell, 2019).\n\n\n\nAdemás de estas métricas, habituales en modelos de clasificación, podemos considerar otras posibles funciones de pérdida.\n\nError de clasificación: Es el error promedio total, que buscamos minimizar. Por ejemplo, asumamos tenemos que clasificar la salida en dos posibles grupos y cada uno consta de 40 observaciones. Si fallamos al clasificar 6 elementos del primer grupo y 3 del segundo, en total hemos fallado en 9 de las 40 observaciones, lo que implica un ratio de clasificación incorrecta del 11,25%.\nError promedio por clase: Este es el error promedio cometido dentro de cada clase. Por ejemplo, en el problema anterior sería \\(6/40\\) en el primer grupo y \\(3/40\\) en el segundo grupo. El propósito es minimizar este error.\nEntropía cruzada (cross-entropy o Log Loss o deviance): esta métrica penaliza en gran medida los casos en los que se asigna una alta probabilidad a una salida incorrecta (es decir, el modelo se dice muy seguro de la respuesta, pero ha fallado). El objetivo también es minimizar su valor.\nÍndice de Gini: empleado en modelos basados en árboles de decisión, es una medida de pureza (purity), donde valores pequeños indican que un nodo contiene, de forma predominante, observaciones de una sola clase. Por tanto, permite medir lo bien que es capaz de separar las clases entre sí el árbol en cada nodo. El objetivo es minimizar su valor.\n\n\n\n4.2.3 Ajuste de parámetros\nComo se ha explicado en la Sección 2.4, cuando escogemos una o varias métricas de evaluación de rendimiento, el procedimiento que solemos seguir es aplicar un método de validación (cruzada con \\(k\\) conjuntos, bootstrapping) para obtener un resultado de evaluación en cada iteración. Finalmente, se combinan estos resultados para obtener una evaluación final de cada modelo, así como unos valores adecuados con los que fijar los hiperparámetros necesarios del modelo.\nEn el caso de los modelos de regresión (salida numérica), este proceso es bastante directo, comparando los resultados de la métrica o métricas seleccionadas. Sin embargo, en el caso de modelos de clasificación la evaluación es más complicada, porque como hemos visto hay que tener en cuenta el rendimiento del modelo para cada uno de los posibles grupos o clases presentes en la variable de salida. Por ese motivo, y debido al hecho de que diferentes combinaciones de los hiperparámetros del modelo ofrecen resultados distintos para la sensibilidad y la especificidad, se prefiere optar por comparar modelos mediante las curvas ROC.\nEn ciertas ocasiones, valdrá la pena, si tenemos tiempo y suficientes recursos computacionales a nuestra disposición, realizar una búsqueda sistemática de los posibles valores de los hiperparámetros de nuestro modelo para garantizar nuestra selección. Esta búsqueda sistemática o grid search implica definir un rango de posibles valores entre los cuales realizaremos un “barrido”, comparando los resultados obtenidos para intentar conseguir un resultado adecuado.",
    "crumbs": [
      "Fundamentos",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Selección y validación de modelos</span>"
    ]
  },
  {
    "objectID": "04-seleccion-validacion.html#comparación-de-modelos",
    "href": "04-seleccion-validacion.html#comparación-de-modelos",
    "title": "4  Selección y validación de modelos",
    "section": "4.3 Comparación de modelos",
    "text": "4.3 Comparación de modelos\n\n4.3.1 Curvas ROC\nLa Figura 4.3 representa gráficamente el llamado ROC convex hull (ROCCH) Fawcett & Provost (2001), que colorea en gris un área cuyo perímetro exterior corresponde a puntos que tienen el mismo rendimiento de clasificaión, desde el punto de vista de la curva ROC.\n\n\n\n\n\n\nFigura 4.3: Representación del convex hull bajo varias curvas ROC, que determina posibles clasificadores óptimos para un problema dado, en función de las restricciones y prioridades consideradas. Fuente: Fig. 7 (Fawcett, 2006).\n\n\n\nPor ejemplo, en la Figura 4.3 a) (izq.) podemos ver que las curvas B y D están dentro de la región del convex hull y, por tanto, son subóptimas para cualquier condición. Por contra, el área principal del convex hull queda delimitada por partes de las curvas A y C. Por lo tanto, si buscamos un clasificador óptimo podríamos eliminar B y D del proceso de selección, directamente. También podemos descartar la utilización de cualquier configuración de los modelos A y C cuyo rendimiento no se encuentre sobre el perímetro del convex hull.\nMatemáticamente, dos puntos en el espacio ROC, \\((FP_1, TP_1)\\) y \\((FP_2, TP_2)\\) tienen un rendimiento equivalente desde el punto de vista de la curva ROC si:\n\\[\n\\frac{TP_2 - TP_1}{FP_2 - FP_1} = m\\,.\n\\]\nEn la Figura 4.3 b) (dcha.), se representan dos líneas explícitasa con rendimiento uniforme, \\(\\alpha\\) y \\(\\beta\\).\nConsideremos un típico problema de clasificación binaria, con dos posibles clases de salida \\(\\{-1, 1\\}\\). En un primer escenario de clasificaión, en el que la clase negativa sobrepasa en número 10:1 a la clase positiva, pero los falsos negativos (FN) y los falsos positivos (FP) tienen el mismo coste, el resultado sería \\(m = 10\\). La línea con pendiente \\(m = 10\\) mas al noroeste (esquina superior izq., mejor rendimiento) del la gráfica ROC es \\(\\alpha\\), tangente al clasificador A. Ese sería el clasificador selecciónado.\nSi ahora tenemos otro escenario de clasificación, en el que la clase negativa y la positiva están balanceadas en número de observaciones, pero un FN es 10 veces más costoso que un FP, entonces el resultado de la ecuación anterior sería \\(m=1/10\\). La curva más al noroeste de la gráfica ROC con esa pendiente es \\(\\beta\\), tangente al clasificador C. Por tanto, ese sería el clasificador más adecuado para este escenario (Fawcett, 2006). En consecuencia, no es inmediato determinar qué clasificador es mejor o peor en términos absolutos, puesto que es imprescindible tener en cuenta las condiciones de contorno del problema (balance de observaciones entre clases, coste de cada error de clasificación, etc.).\nUna forma muy habitual de reducir la información de la curva ROC es calcular el AUC, que hemos definido más arriba. Esta métrica tiene una propiedad interesante: el AUC de un clasificador es equivalente a la probabilidad de que el clasificador asigne un ranking mayor a una instancia positiva elegida al azar que a una instancia negativa elegida también al azar. Por otro lado, el AUC también está relacionado con el coeficiente de Gini (Hand, 2001): \\(\\text{Gini} + 1 = 2 \\times \\text{AUC}\\).\n\n\n4.3.2 Comparación de modelos con R\nVeamos ahora cómo comparar los modelos que hemos ajustado en los ejemplos del Capítulo 3, usando el paquete caret.\n\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\n\nlibrary(caret)\nlibrary(doParallel)\nlibrary(mlbench)\n\ncl &lt;- makePSOCKcluster(5) # 5 cores para proc. paralelo\nregisterDoParallel(cl)\n\n\nload(\"data/sonar-models.RData\")\n\n# Compare model performances using resample()\nmodels_compare &lt;- resamples(list(GLM=model_glm, ADABOOST=model_adaboost, KNN=model_knn,\n                                 XGBOOST=model_xgbLinear, RF=model_rf, GAM_MGCV=model_gam_mgcv, \n                                 GAM_GAM=model_gam, SVM=model_svmRadial))\n\n# Summary of the models performances\nsummary(models_compare)\n\n\nCall:\nsummary.resamples(object = models_compare)\n\nModels: GLM, ADABOOST, KNN, XGBOOST, RF, GAM_MGCV, GAM_GAM, SVM \nNumber of resamples: 5 \n\nROC \n              Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's\nGLM      0.7773109 0.7921569 0.8109244 0.8034314 0.8117647 0.8250000    0\nADABOOST 0.7916667 0.8529412 0.8705882 0.8560364 0.8784314 0.8865546    0\nKNN      0.7773109 0.7875000 0.8078431 0.8092927 0.8130252 0.8607843    0\nXGBOOST  0.8039216 0.8541667 0.8781513 0.8780602 0.9254902 0.9285714    0\nRF       0.7767857 0.8509804 0.8588235 0.8782143 0.9201681 0.9843137    0\nGAM_MGCV 0.7000000 0.7352941 0.7836134 0.7928011 0.8666667 0.8784314    0\nGAM_GAM  0.6974790 0.8705882 0.8791667 0.8616597 0.8862745 0.9747899    0\nSVM      0.7812500 0.8487395 0.8784314 0.8789391 0.8941176 0.9921569    0\n\nSens \n              Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's\nGLM      0.6875000 0.7058824 0.7058824 0.7610294 0.8235294 0.8823529    0\nADABOOST 0.7058824 0.7647059 0.8235294 0.8102941 0.8750000 0.8823529    0\nKNN      0.7058824 0.8125000 0.8235294 0.8213235 0.8823529 0.8823529    0\nXGBOOST  0.6875000 0.7647059 0.8235294 0.7963235 0.8235294 0.8823529    0\nRF       0.7058824 0.7500000 0.8235294 0.8323529 0.8823529 1.0000000    0\nGAM_MGCV 0.6470588 0.7500000 0.7647059 0.7500000 0.7647059 0.8235294    0\nGAM_GAM  0.6470588 0.7647059 0.8235294 0.8110294 0.8823529 0.9375000    0\nSVM      0.6875000 0.7647059 0.8235294 0.8198529 0.8823529 0.9411765    0\n\nSpec \n              Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's\nGLM      0.6000000 0.7333333 0.7857143 0.7828571 0.8666667 0.9285714    0\nADABOOST 0.6428571 0.6666667 0.7857143 0.7523810 0.8000000 0.8666667    0\nKNN      0.5333333 0.5333333 0.7142857 0.6733333 0.7857143 0.8000000    0\nXGBOOST  0.6666667 0.7142857 0.7333333 0.7542857 0.8000000 0.8571429    0\nRF       0.6428571 0.6666667 0.8000000 0.7942857 0.9285714 0.9333333    0\nGAM_MGCV 0.4666667 0.7142857 0.8000000 0.7409524 0.8571429 0.8666667    0\nGAM_GAM  0.6666667 0.7142857 0.7333333 0.7961905 0.8666667 1.0000000    0\nSVM      0.5714286 0.6000000 0.7857143 0.7514286 0.8000000 1.0000000    0\n\n\n\n# Draw box plots to compare models\nscales &lt;- list(x=list(relation=\"free\"), y=list(relation=\"free\"))\nbwplot(models_compare, scales=scales)\n\n\n\n\n\n\n\nFigura 4.4: Comparación gráfica del rendimiento de varios modelos de clasificación candidatos para el problema del dataset mlbench::Sonar. Fuente: [kuhn2013applied].\n\n\n\n\n\nAnálogamente, el paquete caret también incluye funciones que permiten analizar estadísticamente las diferencias entre los modelos, tal y como se explica en la Sec. 5.8.2 del manual de paquete.\n\nstopCluster(cl)\n\n\n\n\n\nBoehmke, B., & Greenwell, B. (2019). Hands-On Machine Learning with R (1.ª ed.). Chapman; Hall/CRC. https://doi.org/10.1201/9780367816377\n\n\nFawcett, T. (2006). An introduction to ROC analysis. Pattern Recognition Letters, 27(8), 861-874. https://doi.org/10.1016/j.patrec.2005.10.010\n\n\nFawcett, T., & Provost, F. (2001). Robust Classification for Imprecise Environments. Machine Learning, 42, 203-231. https://doi.org/10.1023/A:1007601015854\n\n\nHand, T., D. J. (2001). A simple generalization of the area under the ROC curve to multiple class classification problems. Machine Learning, 45(2), 171-186. https://doi.org/10.1023/A:1010920819831",
    "crumbs": [
      "Fundamentos",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Selección y validación de modelos</span>"
    ]
  },
  {
    "objectID": "05-modelos-clasicos.html",
    "href": "05-modelos-clasicos.html",
    "title": "5  Modelos clásicos",
    "section": "",
    "text": "5.1 Problema: predicción de concentración de algas dañinas en ríos\nEn este apartado vamos a presentar un caso de estudio completo en el que podamos ver paso a paso el proceso de entrenamiento y validación de varios modelos de aprendizaje máquina supervisado.\nVamos a utilizar un conjunto de datos del paquete DMwR2, que ya no está disponible en CRAN. También usaremos otro paquete llamado performanceEstimation, del mismo autor que el anterior. Ambos están disponibles en GitHub y podemos instalarlos con el siguiente código.\nUsaremos los siguientes paquetes.\nEl caso de estudio se centra en un conjunto de datos recogidos para estudiar el problema de la proliferación de algas perniciosas en algunos ríos europeos. Las poblaciones de estas algas pueden crecer desmesuradamente en ciertos periodos del año y crean un importante problema para los ecosistemas locales. El caso está desarrollado en (Torgo, 2016), y todo el código de ejemplo está disponible en http://ltorgo.github.io/DMwR2/Ralgae.html.\nConcretamente, se miden varias propiedades químicas del agua (covariables), junto con la frecuencia de aparición de siete especies de algas potencialmente dañinas. También se registran variables con información adicional, como el periodo del año en el que se recogió la muestra, el tamaño del río y la velocidad de sus aguas.\nEl objetivo del caso de estudio es doble:",
    "crumbs": [
      "Métodos convencionales",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Modelos clásicos</span>"
    ]
  },
  {
    "objectID": "05-modelos-clasicos.html#problema-predicción-de-concentración-de-algas-dañinas-en-ríos",
    "href": "05-modelos-clasicos.html#problema-predicción-de-concentración-de-algas-dañinas-en-ríos",
    "title": "5  Modelos clásicos",
    "section": "",
    "text": "Por una parte, intentar abaratar costes de monitorización, creando un modelo que permita guiar sistemas automáticos de recolección de información que puedan alertar sobre la posible creación de situaciones de riesgo en ecosistemas locales.\nPor otra parte, también se pretende entender mejor los factores que puedan estar influyendo en la aparición de estas algas perniciosas, es decir, identificar si estas frecuencias están correladas de alguna forma con las características químicas descriptivas de la muestra tomada o con alguna otra variable (tipo del río, época del año en la que se tomó la muestra, etc.).",
    "crumbs": [
      "Métodos convencionales",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Modelos clásicos</span>"
    ]
  },
  {
    "objectID": "05-modelos-clasicos.html#datasets",
    "href": "05-modelos-clasicos.html#datasets",
    "title": "5  Modelos clásicos",
    "section": "5.2 Datasets",
    "text": "5.2 Datasets\nID: DMwR2::algae.\nLos datos para este estudio provienen de la competición internacional de análisis de datos COIL 1999, y están disponibles a través de diferentes fuentes, incluyendo el UCI Machine Learning Repository.\nEl conjunto de datos contiene en total 200 muestras tomadas de diferentes ríos europeos. Los datos pueden cargarse directamente desde el paquete DMwR2 de la siguiente forma:\n\ndata(algae, package=\"DMwR2\")\nalgae\n\n# A tibble: 200 × 18\n   season size  speed   mxPH  mnO2    Cl    NO3   NH4  oPO4   PO4  Chla    a1\n   &lt;fct&gt;  &lt;fct&gt; &lt;fct&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 winter small medium  8      9.8  60.8  6.24  578   105   170   50      0  \n 2 spring small medium  8.35   8    57.8  1.29  370   429.  559.   1.3    1.4\n 3 autumn small medium  8.1   11.4  40.0  5.33  347.  126.  187.  15.6    3.3\n 4 spring small medium  8.07   4.8  77.4  2.30   98.2  61.2 139.   1.4    3.1\n 5 autumn small medium  8.06   9    55.4 10.4   234.   58.2  97.6 10.5    9.2\n 6 winter small high    8.25  13.1  65.8  9.25  430    18.2  56.7 28.4   15.1\n 7 summer small high    8.15  10.3  73.2  1.54  110    61.2 112.   3.2    2.4\n 8 autumn small high    8.05  10.6  59.1  4.99  206.   44.7  77.4  6.9   18.2\n 9 winter small medium  8.7    3.4  22.0  0.886 103.   36.3  71    5.54  25.4\n10 winter small high    7.93   9.9   8    1.39    5.8  27.2  46.6  0.8   17  \n# ℹ 190 more rows\n# ℹ 6 more variables: a2 &lt;dbl&gt;, a3 &lt;dbl&gt;, a4 &lt;dbl&gt;, a5 &lt;dbl&gt;, a6 &lt;dbl&gt;,\n#   a7 &lt;dbl&gt;\n\n\nCada observación (fila) contiene información sobre 11 variables, tres de las cuales son categóricas y el resto cuantitativas:\n\nseason: Estación del año en la que se tomó la muestra.\nsize: Tamaño del río.\nspeed: Velocidad de las aguas del río.\nmxPH: Máximo valor del PH del río.\nmnO2: Mínimo valor de oxígeno del río.\nCl: Valor medio de cloro.\nNO3: Valor medio de nitratos.\nNH4: Valor medio de amonio.\noPO4: Valor medio de ortofosfato.\nPO4: Valor medio de total de fosfatos.\nChla: Valor medio de clorofila.\na1 hasta a7: Frecuencia de aparición de cada uno de los 7 tipos de algas dañinas analizados en la muestra de agua tomada.\n\nAdicionalmente, existe otro data set en el mismo paquete, llamado test.algae, que contiene otras 140 observaciones adicionales, conteniendo las mismas variables descriptivas, pero que no contiene las columnas correspondientes a los valores de frecuencia de cada especie de algas.\nEn este contexto, podemos usar algae como un conjunto de datos de entrenamiento para nuestro modelo, mientra que test.algae se puede usar como conjunto de datos de test. Finalmente, el dataset algae.sols contiene los valores de las 7 columnas restantes con las frecuencias de aparición de cada especie de algas para cada fila del testing set test.alage. De este modo, podemos evaluar el modelo propuesto.",
    "crumbs": [
      "Métodos convencionales",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Modelos clásicos</span>"
    ]
  },
  {
    "objectID": "05-modelos-clasicos.html#descripción-y-preparación-de-datos",
    "href": "05-modelos-clasicos.html#descripción-y-preparación-de-datos",
    "title": "5  Modelos clásicos",
    "section": "5.3 Descripción y preparación de datos",
    "text": "5.3 Descripción y preparación de datos\n\n5.3.1 Análisis descriptivo\nUn primer resumen descriptivo de las variables implicadas en el análisis es:\n\ndescribe(algae)\n\nalgae \n\n 18  Variables      200  Observations\n--------------------------------------------------------------------------------\nseason \n       n  missing distinct \n     200        0        4 \n                                      \nValue      autumn spring summer winter\nFrequency      40     53     45     62\nProportion  0.200  0.265  0.225  0.310\n--------------------------------------------------------------------------------\nsize \n       n  missing distinct \n     200        0        3 \n                               \nValue       large medium  small\nFrequency      45     84     71\nProportion  0.225  0.420  0.355\n--------------------------------------------------------------------------------\nspeed \n       n  missing distinct \n     200        0        3 \n                               \nValue        high    low medium\nFrequency      84     33     83\nProportion  0.420  0.165  0.415\n--------------------------------------------------------------------------------\nmxPH \n       n  missing distinct     Info     Mean  pMedian      Gmd      .05 \n     199        1       72    0.998    8.012     8.05   0.6471    7.081 \n     .10      .25      .50      .75      .90      .95 \n   7.340    7.700    8.060    8.400    8.700    8.873 \n\nlowest : 5.6  5.7  6.4  6.5  6.6 , highest: 9    9.06 9.1  9.5  9.7 \n--------------------------------------------------------------------------------\nmnO2 \n       n  missing distinct     Info     Mean  pMedian      Gmd      .05 \n     198        2       88        1    9.118     9.35    2.629    4.485 \n     .10      .25      .50      .75      .90      .95 \n   5.770    7.725    9.800   10.800   11.700   11.815 \n\nlowest : 1.5  1.8  3.2  3.3  3.4 , highest: 12.5 12.6 12.9 13.1 13.4\n--------------------------------------------------------------------------------\nCl \n       n  missing distinct     Info     Mean  pMedian      Gmd      .05 \n     190       10      178        1    43.64    36.38    43.78    3.061 \n     .10      .25      .50      .75      .90      .95 \n   4.970   10.981   32.730   57.823   88.600  130.087 \n\nlowest : 0.222   0.8     1.17    1.45    1.549  \nhighest: 173.75  187.183 194.75  208.364 391.5  \n--------------------------------------------------------------------------------\nNO3 \n       n  missing distinct     Info     Mean  pMedian      Gmd      .05 \n     198        2      192        1    3.282    2.867    2.884   0.4023 \n     .10      .25      .50      .75      .90      .95 \n  0.6912   1.2960   2.6750   4.4463   6.1916   7.9369 \n\nlowest : 0.05   0.102  0.13   0.23   0.267 , highest: 9.248  9.715  9.773  10.416 45.65 \n--------------------------------------------------------------------------------\nNH4 \n       n  missing distinct     Info     Mean  pMedian      Gmd      .05 \n     198        2      179        1    501.3      131    816.2    10.00 \n     .10      .25      .50      .75      .90      .95 \n   15.00    38.33   103.17   226.95   805.33  1922.87 \n\nlowest : 5       5.8     8       10      10.5   \nhighest: 4073.33 5738.33 6400    8777.6  24064  \n--------------------------------------------------------------------------------\noPO4 \n       n  missing distinct     Info     Mean  pMedian      Gmd      .05 \n     198        2      173        1    73.59    52.77    85.46     2.00 \n     .10      .25      .50      .75      .90      .95 \n    3.94    15.70    40.15    99.33   193.21   248.34 \n\nlowest : 1       1.25    1.333   1.625   1.8    \nhighest: 346.167 412.333 428.75  467.5   564.6  \n--------------------------------------------------------------------------------\nPO4 \n       n  missing distinct     Info     Mean  pMedian      Gmd      .05 \n     198        2      189        1    137.9      125    133.9    6.455 \n     .10      .25      .50      .75      .90      .95 \n  11.350   41.375  103.285  213.750  286.100  345.650 \n\nlowest : 1       2.5     3       4       6      \nhighest: 558.75  586     607.167 624.733 771.6  \n--------------------------------------------------------------------------------\nChla \n       n  missing distinct     Info     Mean  pMedian      Gmd      .05 \n     188       12      131        1    13.97     9.35    17.93    0.500 \n     .10      .25      .50      .75      .90      .95 \n   0.800    2.000    5.475   18.308   31.817   61.733 \n\nlowest : 0.2     0.3     0.4     0.5     0.6    \nhighest: 88.255  92.667  93.683  98.817  110.456\n--------------------------------------------------------------------------------\na1 \n       n  missing distinct     Info     Mean  pMedian      Gmd      .05 \n     200        0      121    0.994    16.92    12.45    21.52     0.00 \n     .10      .25      .50      .75      .90      .95 \n    0.00     1.50     6.95    24.80    50.72    64.33 \n\nlowest : 0    1.1  1.2  1.4  1.5 , highest: 75.8 81.9 82.7 86.6 89.8\n--------------------------------------------------------------------------------\na2 \n       n  missing distinct     Info     Mean  pMedian      Gmd      .05 \n     200        0       89    0.951    7.458     5.25    10.19     0.00 \n     .10      .25      .50      .75      .90      .95 \n    0.00     0.00     3.00    11.38    21.50    28.38 \n\nlowest : 0    1    1.2  1.4  1.5 , highest: 40.7 40.9 41   53.6 72.6\n--------------------------------------------------------------------------------\na3 \n       n  missing distinct     Info     Mean  pMedian      Gmd      .05 \n     200        0       79    0.949    4.309      2.4    6.131    0.000 \n     .10      .25      .50      .75      .90      .95 \n   0.000    0.000    1.550    4.925   13.510   20.275 \n\nlowest : 0    1    1.1  1.2  1.4 , highest: 24.8 25.3 25.9 35.1 42.8\n--------------------------------------------------------------------------------\na4 \n       n  missing distinct     Info     Mean  pMedian      Gmd      .05 \n     200        0       50    0.838    1.992      1.1    3.032    0.000 \n     .10      .25      .50      .75      .90      .95 \n   0.000    0.000    0.000    2.400    5.000    7.605 \n\nlowest : 0    1    1.1  1.2  1.3 , highest: 11.5 12.7 13.4 28.8 44.6\n--------------------------------------------------------------------------------\na5 \n       n  missing distinct     Info     Mean  pMedian      Gmd      .05 \n     200        0       81    0.938    5.064      3.7    6.923     0.00 \n     .10      .25      .50      .75      .90      .95 \n    0.00     0.00     1.90     7.50    14.91    20.04 \n\nlowest : 0    1    1.1  1.2  1.4 , highest: 28.8 34.2 34.3 35.6 44.4\n--------------------------------------------------------------------------------\na6 \n       n  missing distinct     Info     Mean  pMedian      Gmd      .05 \n     200        0       76    0.847    5.964     2.95    9.323    0.000 \n     .10      .25      .50      .75      .90      .95 \n   0.000    0.000    0.000    6.925   17.110   31.815 \n\nlowest : 0    1    1.2  1.4  1.5 , highest: 42.7 49.4 52.5 64.6 77.6\n--------------------------------------------------------------------------------\na7 \n       n  missing distinct     Info     Mean  pMedian      Gmd      .05 \n     200        0       51    0.882    2.496      1.2    3.817     0.00 \n     .10      .25      .50      .75      .90      .95 \n    0.00     0.00     1.00     2.40     6.10    10.88 \n\nlowest : 0    1    1.1  1.2  1.4 , highest: 22.1 25.6 30.1 31.2 31.6\n--------------------------------------------------------------------------------\n\n\nTambién podemos obtener varios gráficos que permiten hacernos una idea de la posible correlación entre algunas de las variables de nuestro conjunto de datos. Estos ejemplos los podemos encontrar en el epígrafe “Data Visualization and Summarization” de la página que contiene el código original para el caso de estudio.\nParticularmente importantes son los comandos para organizar las etiquetas de las variables categóricas siguiendo un orden en particular:\n\nalgae &lt;- algae |&gt;\n  mutate(size=fct_relevel(size,c(\"small\",\"medium\",\"large\")),\n         speed=fct_relevel(speed,c(\"low\",\"medium\",\"high\")),\n         season=fct_relevel(season,c(\"spring\",\"summer\",\"autumn\",\"winter\")))\n\nDonde usamos la biblioteca forcats, de reciente creación y parte del metapaquete tidyverse (con paquetes asociados al concepto de Tidy Data promulgado por H. Wickham). Este paquete contiene funciones muy útiles para operar con variables categóricas en el contexto de tidy data 1.\nTambién es interesante representar el valor de la concentración de algas en función de alguna de las variables categóricas, de forma que podamos explorar posibles correlaciones:\n\ndata2graph &lt;- filter(algae,!is.na(mnO2)) |&gt;\n    mutate(minO2=cut(mnO2, quantile(mnO2,c(0,0.25,.5,.75,1)), include.lowest=TRUE))\nggplot(data2graph,aes(x=a3,y=season, color=season)) + geom_point() + \n    facet_wrap(~ minO2) + \n    guides(color=\"none\") +\n  ggtitle(\"Cdplot de freq. alga a3 vs. estación del año\")\n\n\n\n\n\n\n\nFigura 5.1: Caption\n\n\n\n\n\n\nlibrary(corrplot)\ncm &lt;- cor(algae[,4:18], use=\"complete.obs\")\ncorrplot(cm, type=\"upper\", tl.pos=\"d\")\ncorrplot(cm, add=TRUE, type=\"lower\",method=\"number\", \n         diag=FALSE, tl.pos=\"n\", cl.pos=\"n\")\n\n\n\n\n\n\n\nFigura 5.2\n\n\n\n\n\n\n\n5.3.2 Imputación de datos faltantes\nLas siguientes secciones del ejemplo muestran código paso a paso para implementar algunas de las técnicas de imputación que hemos visto en este documento. A veces encontraremos funciones de utilidad que el autor del paquete ha creado para automatizar métodos de imputación avanzados, por ejemplo knnImputation para imputación de valores faltantes utilizando información de los \\(k\\) vecinos más próximos a un punto dado.\n\nEliminación de casos con valores faltantes.\nImputación con valores más frecuentes.\nImputación mediante exploración de correlaciones entre los datos.\nImputación de datos mediante búsqueda de similiaridad.",
    "crumbs": [
      "Métodos convencionales",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Modelos clásicos</span>"
    ]
  },
  {
    "objectID": "05-modelos-clasicos.html#ajuste-de-modelosalgoritmos",
    "href": "05-modelos-clasicos.html#ajuste-de-modelosalgoritmos",
    "title": "5  Modelos clásicos",
    "section": "5.4 Ajuste de modelos/algoritmos",
    "text": "5.4 Ajuste de modelos/algoritmos\nEn las siguientes secciones, el ejemplo continua desarrollando varios tipos de modelos para los datos disponibles:\n\nUn modelo de predicción basado en regresión lineal múltiple.\nUn modelo orientado a inferencia, creando árboles de regresión.\nUn algoritmo de ensamblado de modelos, Random Forests.\n\n\n5.4.1 Regresión lineal múltiple\nPara este caso, en primer lugar debemos de rellenar los datos faltantes con algún método de imputación de datos. Aquí se utiliza el método de fijarnos en las obseraciones del dataset de entrenamiento que son similares a las que contienen datos faltantes para rellenar los valores desconocidos.\n\nalgae &lt;-  algae[-manyNAs(algae), ]\nclean.algae &lt;- knnImputation(algae, k = 10)\n\nComenzamos por ajustar un modelo saturado, es decir, con todas las posibles variables que se han observado, para ir simplificándolo paulatinamente.\n\nlm_a1 &lt;- lm(a1 ~ ., data = clean.algae[, 1:12])\nsummary(lm_a1)\n\n\nCall:\nlm(formula = a1 ~ ., data = clean.algae[, 1:12])\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-37.692 -11.894  -2.531   7.359  62.178 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)  60.380064  21.919304   2.755  0.00647 **\nseasonsummer -2.985251   3.742399  -0.798  0.42609   \nseasonautumn -3.707641   4.135847  -0.896  0.37119   \nseasonwinter -0.010954   3.394378  -0.003  0.99743   \nsizemedium   -6.401875   3.447182  -1.857  0.06491 . \nsizelarge    -9.675740   4.175134  -2.317  0.02159 * \nspeedmedium  -3.701776   4.107879  -0.901  0.36870   \nspeedhigh    -3.958606   4.701947  -0.842  0.40094   \nmxPH         -3.596932   2.704224  -1.330  0.18514   \nmnO2          1.053401   0.704865   1.494  0.13678   \nCl           -0.040892   0.033645  -1.215  0.22578   \nNO3          -1.510771   0.551190  -2.741  0.00674 **\nNH4           0.001632   0.001003   1.628  0.10533   \noPO4         -0.005208   0.039867  -0.131  0.89620   \nPO4          -0.052335   0.030742  -1.702  0.09039 . \nChla         -0.088703   0.079962  -1.109  0.26876   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 17.64 on 182 degrees of freedom\nMultiple R-squared:  0.3736,    Adjusted R-squared:  0.3219 \nF-statistic: 7.236 on 15 and 182 DF,  p-value: 2.314e-12\n\n\nDespués, se puede utilizar un análisis ANOVA para efectuar un análisis secuencial de la construcción del modelo, viendo cómo se reduce el RSS (error total) conforme añadimos términos a la fórmula en cada paso.\n\nanova(lm_a1)\n\nAnalysis of Variance Table\n\nResponse: a1\n           Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nseason      3     85    28.2  0.0906 0.9651608    \nsize        2  11401  5700.7 18.3213 5.632e-08 ***\nspeed       2   3934  1967.2  6.3222 0.0022155 ** \nmxPH        1   1313  1312.8  4.2193 0.0413968 *  \nmnO2        1   2297  2296.5  7.3807 0.0072288 ** \nCl          1   4361  4360.7 14.0147 0.0002431 ***\nNO3         1   3409  3409.3 10.9569 0.0011244 ** \nNH4         1    403   403.4  1.2965 0.2563459    \noPO4        1   4772  4772.1 15.3367 0.0001271 ***\nPO4         1   1413  1413.2  4.5418 0.0344180 *  \nChla        1    383   382.9  1.2306 0.2687586    \nResiduals 182  56630   311.2                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nseason es la variable que menos contribuye a reducir el error total, así que la eliminamos.\n\nlm2_a1 &lt;- update(lm_a1, . ~ . - season)\nsummary(lm2_a1)\n\n\nCall:\nlm(formula = a1 ~ size + speed + mxPH + mnO2 + Cl + NO3 + NH4 + \n    oPO4 + PO4 + Chla, data = clean.algae[, 1:12])\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-36.466 -11.774  -2.987   7.428  63.708 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)  5.832e+01  2.167e+01   2.691  0.00778 **\nsizemedium  -6.951e+00  3.390e+00  -2.051  0.04172 * \nsizelarge   -1.027e+01  4.117e+00  -2.495  0.01346 * \nspeedmedium -3.380e+00  4.077e+00  -0.829  0.40821   \nspeedhigh   -3.094e+00  4.607e+00  -0.672  0.50271   \nmxPH        -3.267e+00  2.658e+00  -1.229  0.22065   \nmnO2         8.022e-01  6.587e-01   1.218  0.22486   \nCl          -3.892e-02  3.337e-02  -1.166  0.24501   \nNO3         -1.532e+00  5.475e-01  -2.798  0.00568 **\nNH4          1.576e-03  9.947e-04   1.585  0.11475   \noPO4        -6.010e-03  3.949e-02  -0.152  0.87921   \nPO4         -5.106e-02  3.051e-02  -1.674  0.09586 . \nChla        -8.489e-02  7.942e-02  -1.069  0.28653   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 17.57 on 185 degrees of freedom\nMultiple R-squared:  0.3686,    Adjusted R-squared:  0.3276 \nF-statistic:     9 on 12 and 185 DF,  p-value: 1.668e-13\n\n\nVolvemos a usar ANOVA, pero ahora para comprobar la mejora entre los dos modelos.\n\nanova(lm_a1, lm2_a1)\n\nAnalysis of Variance Table\n\nModel 1: a1 ~ season + size + speed + mxPH + mnO2 + Cl + NO3 + NH4 + oPO4 + \n    PO4 + Chla\nModel 2: a1 ~ size + speed + mxPH + mnO2 + Cl + NO3 + NH4 + oPO4 + PO4 + \n    Chla\n  Res.Df   RSS Df Sum of Sq     F Pr(&gt;F)\n1    182 56630                          \n2    185 57079 -3   -449.02 0.481 0.6959\n\n\nLa función step() permite realizar una eliminación secuencial hacia atras (backward elimination) para simplificar el modelo, usando en cada paso el Akaike Information Criterion (AIC) para identificar la variable candidata a ser eliminada.\n\n\nStart:  AIC=1151.89\na1 ~ season + size + speed + mxPH + mnO2 + Cl + NO3 + NH4 + oPO4 + \n    PO4 + Chla\n\n         Df Sum of Sq   RSS    AIC\n- season  3    449.02 57079 1147.5\n- speed   2    274.12 56904 1148.8\n- oPO4    1      5.31 56635 1149.9\n- Chla    1    382.90 57013 1151.2\n- Cl      1    459.64 57090 1151.5\n- mxPH    1    550.50 57180 1151.8\n&lt;none&gt;                56630 1151.9\n- mnO2    1    694.94 57325 1152.3\n- NH4     1    824.31 57454 1152.8\n- PO4     1    901.78 57532 1153.0\n- size    2   1855.68 58486 1154.3\n- NO3     1   2337.60 58968 1157.9\n\nStep:  AIC=1147.46\na1 ~ size + speed + mxPH + mnO2 + Cl + NO3 + NH4 + oPO4 + PO4 + \n    Chla\n\n        Df Sum of Sq   RSS    AIC\n- speed  2    214.41 57293 1144.2\n- oPO4   1      7.15 57086 1145.5\n- Chla   1    352.48 57431 1146.7\n- Cl     1    419.66 57499 1146.9\n- mnO2   1    457.55 57537 1147.0\n- mxPH   1    465.99 57545 1147.1\n&lt;none&gt;               57079 1147.5\n- NH4    1    774.78 57854 1148.1\n- PO4    1    864.40 57943 1148.4\n- size   2   2177.14 59256 1150.9\n- NO3    1   2416.12 59495 1153.7\n\nStep:  AIC=1144.2\na1 ~ size + mxPH + mnO2 + Cl + NO3 + NH4 + oPO4 + PO4 + Chla\n\n       Df Sum of Sq   RSS    AIC\n- oPO4  1     15.75 57309 1142.2\n- Chla  1    227.71 57521 1143.0\n- mnO2  1    413.56 57707 1143.6\n- mxPH  1    480.09 57773 1143.8\n- Cl    1    489.31 57783 1143.9\n&lt;none&gt;              57293 1144.2\n- NH4   1    718.17 58012 1144.7\n- PO4   1    810.82 58104 1145.0\n- size  2   2058.91 59352 1147.2\n- NO3   1   2373.68 59667 1150.2\n\nStep:  AIC=1142.25\na1 ~ size + mxPH + mnO2 + Cl + NO3 + NH4 + PO4 + Chla\n\n       Df Sum of Sq   RSS    AIC\n- Chla  1     212.9 57522 1141.0\n- mnO2  1     402.6 57712 1141.6\n- Cl    1     487.1 57796 1141.9\n- mxPH  1     516.3 57825 1142.0\n&lt;none&gt;              57309 1142.2\n- NH4   1     702.9 58012 1142.7\n- size  2    2049.1 59358 1145.2\n- NO3   1    2364.8 59674 1148.3\n- PO4   1    5802.7 63112 1159.3\n\nStep:  AIC=1140.99\na1 ~ size + mxPH + mnO2 + Cl + NO3 + NH4 + PO4\n\n       Df Sum of Sq   RSS    AIC\n- mnO2  1     434.9 57957 1140.5\n- Cl    1     452.4 57974 1140.5\n&lt;none&gt;              57522 1141.0\n- NH4   1     745.8 58268 1141.5\n- mxPH  1     834.5 58357 1141.8\n- size  2    2217.8 59740 1144.5\n- NO3   1    2663.9 60186 1148.0\n- PO4   1    6296.4 63818 1159.5\n\nStep:  AIC=1140.48\na1 ~ size + mxPH + Cl + NO3 + NH4 + PO4\n\n       Df Sum of Sq   RSS    AIC\n- NH4   1     530.2 58487 1140.3\n&lt;none&gt;              57957 1140.5\n- Cl    1     601.4 58558 1140.5\n- mxPH  1     818.8 58776 1141.3\n- size  2    2480.4 60437 1144.8\n- NO3   1    2248.6 60206 1146.0\n- PO4   1    9079.2 67036 1167.3\n\nStep:  AIC=1140.28\na1 ~ size + mxPH + Cl + NO3 + PO4\n\n       Df Sum of Sq   RSS    AIC\n&lt;none&gt;              58487 1140.3\n- mxPH  1     782.6 59270 1140.9\n- Cl    1     855.1 59342 1141.2\n- NO3   1    1985.7 60473 1144.9\n- size  2    2667.0 61154 1145.1\n- PO4   1    8557.9 67045 1165.3\n\n\nInspeccionamos el modelo final.\n\nsummary(final_lm)\n\n\nCall:\nlm(formula = a1 ~ size + mxPH + Cl + NO3 + PO4, data = clean.algae[, \n    1:12])\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-28.876 -12.707  -3.718   8.413  62.910 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  67.66988   19.18518   3.527 0.000526 ***\nsizemedium   -7.60277    3.14190  -2.420 0.016466 *  \nsizelarge   -10.40755    3.81939  -2.725 0.007029 ** \nmxPH         -3.96549    2.48058  -1.599 0.111560    \nCl           -0.05289    0.03165  -1.671 0.096337 .  \nNO3          -0.89491    0.35143  -2.547 0.011669 *  \nPO4          -0.05906    0.01117  -5.287 3.39e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 17.5 on 191 degrees of freedom\nMultiple R-squared:  0.353, Adjusted R-squared:  0.3327 \nF-statistic: 17.37 on 6 and 191 DF,  p-value: 5.299e-16\n\n\nObservamos que el \\(R_{adj}^2\\) es tan sólo \\(0.3324\\), un resultado cuanto menos demasiado modesto. Entre otras posibles causas, esto puede indicar que los presupuestos de linealidad que exige este modelo tan sencillo no son aplicables a este problema.\n\n\n5.4.2 Árboles de regresión\nEn el caso del modelo orientado a inferencia estadística, la construcción se centra en la variable de salida a1.\n\ndata(algae, package=\"DMwR2\") \nalgae &lt;- algae[-manyNAs(algae), ]\nrt_a1 &lt;- rpart(a1 ~ ., data = algae[, 1:12])\nrt_a1\n\nn= 198 \n\nnode), split, n, deviance, yval\n      * denotes terminal node\n\n 1) root 198 90401.290 16.996460  \n   2) PO4&gt;=43.818 147 31279.120  8.979592  \n     4) Cl&gt;=7.8065 140 21622.830  7.492857  \n       8) oPO4&gt;=51.118 84  3441.149  3.846429 *\n       9) oPO4&lt; 51.118 56 15389.430 12.962500  \n        18) mnO2&gt;=10.05 24  1248.673  6.716667 *\n        19) mnO2&lt; 10.05 32 12502.320 17.646870  \n          38) NO3&gt;=3.1875 9   257.080  7.866667 *\n          39) NO3&lt; 3.1875 23 11047.500 21.473910  \n            78) mnO2&lt; 8 13  2919.549 13.807690 *\n            79) mnO2&gt;=8 10  6370.704 31.440000 *\n     5) Cl&lt; 7.8065 7  3157.769 38.714290 *\n   3) PO4&lt; 43.818 51 22442.760 40.103920  \n     6) mxPH&lt; 7.87 28 11452.770 33.450000  \n      12) mxPH&gt;=7.045 18  5146.169 26.394440 *\n      13) mxPH&lt; 7.045 10  3797.645 46.150000 *\n     7) mxPH&gt;=7.87 23  8241.110 48.204350  \n      14) PO4&gt;=15.177 12  3047.517 38.183330 *\n      15) PO4&lt; 15.177 11  2673.945 59.136360 *\n\n# summary(rt_a1) # Produce mucha información\n\n\n# Función rpart.plot::prp\nprp(rt_a1, extra=101, box.col=\"orange\", split.box.col=\"grey\")\n\n\n\n\n\n\n\nFigura 5.3: Cálculo y representación de un modelo de árbol de regresión para el dataset algae.\n\n\n\n\n\nPartiendo de este árbol complejo y profundo podemos simplificar el modelo podándolo mediante el criterio de cost complexity, implementado en el paquete rpart. El método de poda intenta seguir una solución de compromiso entre la capacidad predictiva del modelo y y el tamaño del árbol. Utilizando este criterio, R puede calcular subconjuntos del árbol original estimando su capacidad predictiva. Esta información está disponible mediante la función printcp().\n\nprintcp(rt_a1)\n\n\nRegression tree:\nrpart(formula = a1 ~ ., data = algae[, 1:12])\n\nVariables actually used in tree construction:\n[1] Cl   mnO2 mxPH NO3  oPO4 PO4 \n\nRoot node error: 90401/198 = 456.57\n\nn= 198 \n\n        CP nsplit rel error  xerror    xstd\n1 0.405740      0   1.00000 1.00763 0.13040\n2 0.071885      1   0.59426 0.74669 0.12047\n3 0.030887      2   0.52237 0.69082 0.11873\n4 0.030408      3   0.49149 0.69103 0.11905\n5 0.027872      4   0.46108 0.69283 0.12042\n6 0.027754      5   0.43321 0.68221 0.11496\n7 0.018124      6   0.40545 0.66712 0.11167\n8 0.016344      7   0.38733 0.74325 0.12261\n9 0.010000      9   0.35464 0.73604 0.11837\n\n\nSegún (Torgo, 2016), se pueden seguir varios criterios para seleccionar el árbol más adecuado:\n\nSeleccionando aquel que tenga el menor error relativo estimado (columna xerror en la salida). En este caso sería el tercer árbol.\nSeleccionando aquel que tenga el menor error de validación cruzada estimado (columna xerror) más 1-SE (columna xstd). En este caso, sería el árbol más pequeño cuyo valor en la columna xerror sea menor que \\(0.67151 + 0.11508\\). Se seleccionaría el árbol número 2.\n\nSi nos quedamos con el árbol número dos, usamo el valor de cp = 0.08.\n\nrt2_a1 &lt;- prune(rt_a1, cp = 0.08)\nrt2_a1 \n\nn= 198 \n\nnode), split, n, deviance, yval\n      * denotes terminal node\n\n1) root 198 90401.29 16.996460  \n  2) PO4&gt;=43.818 147 31279.12  8.979592 *\n  3) PO4&lt; 43.818 51 22442.76 40.103920 *\n\n\nLa función DMwR2::rpartXse automatiza todo este proceso.\n\n\n5.4.3 Random Forests\nComparemos ahora lo que ocurre cuando enfrentamos el modelo de regresión lineal múltiple y el modelo basado en árbol de decisión con un algoritmo más robusto y avanzado, como Random Forests (RF), basado en un método de ensamblado de árboles de decisión aleatorizados. El paquete performanceEstimation permite utilizar una sintáxis muy explícita para configurar el proceso de ajuste y comparación de modelos.\nLa métrica utilizada en este caso es el NMSE (Normalised MSE), que se obtiene calculando el cociente entre el rendimiento de nuestro modelo y el de un modelo de base (baseline) como, por ejemplo, el valor promedio de la salida (null model).\n\n\n\n\n##### PERFORMANCE ESTIMATION USING  CROSS VALIDATION  #####\n\n** PREDICTIVE TASK :: a1\n\n++ MODEL/WORKFLOW :: lm \nTask for estimating  nmse  using\n 5 x 10 - Fold Cross Validation\n     Run with seed =  1234 \nIteration :**************************************************\n\n\n++ MODEL/WORKFLOW :: rpartXse.v1 \nTask for estimating  nmse  using\n 5 x 10 - Fold Cross Validation\n     Run with seed =  1234 \nIteration :**************************************************\n\n\n++ MODEL/WORKFLOW :: rpartXse.v2 \nTask for estimating  nmse  using\n 5 x 10 - Fold Cross Validation\n     Run with seed =  1234 \nIteration :**************************************************\n\n\n++ MODEL/WORKFLOW :: rpartXse.v3 \nTask for estimating  nmse  using\n 5 x 10 - Fold Cross Validation\n     Run with seed =  1234 \nIteration :**************************************************\n\n\n++ MODEL/WORKFLOW :: randomForest.v1 \nTask for estimating  nmse  using\n 5 x 10 - Fold Cross Validation\n     Run with seed =  1234 \nIteration :**************************************************\n\n\n++ MODEL/WORKFLOW :: randomForest.v2 \nTask for estimating  nmse  using\n 5 x 10 - Fold Cross Validation\n     Run with seed =  1234 \nIteration :**************************************************\n\n\n++ MODEL/WORKFLOW :: randomForest.v3 \nTask for estimating  nmse  using\n 5 x 10 - Fold Cross Validation\n     Run with seed =  1234 \nIteration :**************************************************\n\n\n** PREDICTIVE TASK :: a2\n\n++ MODEL/WORKFLOW :: lm \nTask for estimating  nmse  using\n 5 x 10 - Fold Cross Validation\n     Run with seed =  1234 \nIteration :**************************************************\n\n\n++ MODEL/WORKFLOW :: rpartXse.v1 \nTask for estimating  nmse  using\n 5 x 10 - Fold Cross Validation\n     Run with seed =  1234 \nIteration :**************************************************\n\n\n++ MODEL/WORKFLOW :: rpartXse.v2 \nTask for estimating  nmse  using\n 5 x 10 - Fold Cross Validation\n     Run with seed =  1234 \nIteration :**************************************************\n\n\n++ MODEL/WORKFLOW :: rpartXse.v3 \nTask for estimating  nmse  using\n 5 x 10 - Fold Cross Validation\n     Run with seed =  1234 \nIteration :**************************************************\n\n\n++ MODEL/WORKFLOW :: randomForest.v1 \nTask for estimating  nmse  using\n 5 x 10 - Fold Cross Validation\n     Run with seed =  1234 \nIteration :**************************************************\n\n\n++ MODEL/WORKFLOW :: randomForest.v2 \nTask for estimating  nmse  using\n 5 x 10 - Fold Cross Validation\n     Run with seed =  1234 \nIteration :**************************************************\n\n\n++ MODEL/WORKFLOW :: randomForest.v3 \nTask for estimating  nmse  using\n 5 x 10 - Fold Cross Validation\n     Run with seed =  1234 \nIteration :**************************************************\n\n\n** PREDICTIVE TASK :: a3\n\n++ MODEL/WORKFLOW :: lm \nTask for estimating  nmse  using\n 5 x 10 - Fold Cross Validation\n     Run with seed =  1234 \nIteration :**************************************************\n\n\n++ MODEL/WORKFLOW :: rpartXse.v1 \nTask for estimating  nmse  using\n 5 x 10 - Fold Cross Validation\n     Run with seed =  1234 \nIteration :**************************************************\n\n\n++ MODEL/WORKFLOW :: rpartXse.v2 \nTask for estimating  nmse  using\n 5 x 10 - Fold Cross Validation\n     Run with seed =  1234 \nIteration :**************************************************\n\n\n++ MODEL/WORKFLOW :: rpartXse.v3 \nTask for estimating  nmse  using\n 5 x 10 - Fold Cross Validation\n     Run with seed =  1234 \nIteration :**************************************************\n\n\n++ MODEL/WORKFLOW :: randomForest.v1 \nTask for estimating  nmse  using\n 5 x 10 - Fold Cross Validation\n     Run with seed =  1234 \nIteration :**************************************************\n\n\n++ MODEL/WORKFLOW :: randomForest.v2 \nTask for estimating  nmse  using\n 5 x 10 - Fold Cross Validation\n     Run with seed =  1234 \nIteration :**************************************************\n\n\n++ MODEL/WORKFLOW :: randomForest.v3 \nTask for estimating  nmse  using\n 5 x 10 - Fold Cross Validation\n     Run with seed =  1234 \nIteration :**************************************************\n\n\n** PREDICTIVE TASK :: a4\n\n++ MODEL/WORKFLOW :: lm \nTask for estimating  nmse  using\n 5 x 10 - Fold Cross Validation\n     Run with seed =  1234 \nIteration :**************************************************\n\n\n++ MODEL/WORKFLOW :: rpartXse.v1 \nTask for estimating  nmse  using\n 5 x 10 - Fold Cross Validation\n     Run with seed =  1234 \nIteration :**************************************************\n\n\n++ MODEL/WORKFLOW :: rpartXse.v2 \nTask for estimating  nmse  using\n 5 x 10 - Fold Cross Validation\n     Run with seed =  1234 \nIteration :**************************************************\n\n\n++ MODEL/WORKFLOW :: rpartXse.v3 \nTask for estimating  nmse  using\n 5 x 10 - Fold Cross Validation\n     Run with seed =  1234 \nIteration :**************************************************\n\n\n++ MODEL/WORKFLOW :: randomForest.v1 \nTask for estimating  nmse  using\n 5 x 10 - Fold Cross Validation\n     Run with seed =  1234 \nIteration :**************************************************\n\n\n++ MODEL/WORKFLOW :: randomForest.v2 \nTask for estimating  nmse  using\n 5 x 10 - Fold Cross Validation\n     Run with seed =  1234 \nIteration :**************************************************\n\n\n++ MODEL/WORKFLOW :: randomForest.v3 \nTask for estimating  nmse  using\n 5 x 10 - Fold Cross Validation\n     Run with seed =  1234 \nIteration :**************************************************\n\n\n** PREDICTIVE TASK :: a5\n\n++ MODEL/WORKFLOW :: lm \nTask for estimating  nmse  using\n 5 x 10 - Fold Cross Validation\n     Run with seed =  1234 \nIteration :**************************************************\n\n\n++ MODEL/WORKFLOW :: rpartXse.v1 \nTask for estimating  nmse  using\n 5 x 10 - Fold Cross Validation\n     Run with seed =  1234 \nIteration :**************************************************\n\n\n++ MODEL/WORKFLOW :: rpartXse.v2 \nTask for estimating  nmse  using\n 5 x 10 - Fold Cross Validation\n     Run with seed =  1234 \nIteration :**************************************************\n\n\n++ MODEL/WORKFLOW :: rpartXse.v3 \nTask for estimating  nmse  using\n 5 x 10 - Fold Cross Validation\n     Run with seed =  1234 \nIteration :**************************************************\n\n\n++ MODEL/WORKFLOW :: randomForest.v1 \nTask for estimating  nmse  using\n 5 x 10 - Fold Cross Validation\n     Run with seed =  1234 \nIteration :**************************************************\n\n\n++ MODEL/WORKFLOW :: randomForest.v2 \nTask for estimating  nmse  using\n 5 x 10 - Fold Cross Validation\n     Run with seed =  1234 \nIteration :**************************************************\n\n\n++ MODEL/WORKFLOW :: randomForest.v3 \nTask for estimating  nmse  using\n 5 x 10 - Fold Cross Validation\n     Run with seed =  1234 \nIteration :**************************************************\n\n\n** PREDICTIVE TASK :: a6\n\n++ MODEL/WORKFLOW :: lm \nTask for estimating  nmse  using\n 5 x 10 - Fold Cross Validation\n     Run with seed =  1234 \nIteration :**************************************************\n\n\n++ MODEL/WORKFLOW :: rpartXse.v1 \nTask for estimating  nmse  using\n 5 x 10 - Fold Cross Validation\n     Run with seed =  1234 \nIteration :**************************************************\n\n\n++ MODEL/WORKFLOW :: rpartXse.v2 \nTask for estimating  nmse  using\n 5 x 10 - Fold Cross Validation\n     Run with seed =  1234 \nIteration :**************************************************\n\n\n++ MODEL/WORKFLOW :: rpartXse.v3 \nTask for estimating  nmse  using\n 5 x 10 - Fold Cross Validation\n     Run with seed =  1234 \nIteration :**************************************************\n\n\n++ MODEL/WORKFLOW :: randomForest.v1 \nTask for estimating  nmse  using\n 5 x 10 - Fold Cross Validation\n     Run with seed =  1234 \nIteration :**************************************************\n\n\n++ MODEL/WORKFLOW :: randomForest.v2 \nTask for estimating  nmse  using\n 5 x 10 - Fold Cross Validation\n     Run with seed =  1234 \nIteration :**************************************************\n\n\n++ MODEL/WORKFLOW :: randomForest.v3 \nTask for estimating  nmse  using\n 5 x 10 - Fold Cross Validation\n     Run with seed =  1234 \nIteration :**************************************************\n\n\n** PREDICTIVE TASK :: a7\n\n++ MODEL/WORKFLOW :: lm \nTask for estimating  nmse  using\n 5 x 10 - Fold Cross Validation\n     Run with seed =  1234 \nIteration :**************************************************\n\n\n++ MODEL/WORKFLOW :: rpartXse.v1 \nTask for estimating  nmse  using\n 5 x 10 - Fold Cross Validation\n     Run with seed =  1234 \nIteration :**************************************************\n\n\n++ MODEL/WORKFLOW :: rpartXse.v2 \nTask for estimating  nmse  using\n 5 x 10 - Fold Cross Validation\n     Run with seed =  1234 \nIteration :**************************************************\n\n\n++ MODEL/WORKFLOW :: rpartXse.v3 \nTask for estimating  nmse  using\n 5 x 10 - Fold Cross Validation\n     Run with seed =  1234 \nIteration :**************************************************\n\n\n++ MODEL/WORKFLOW :: randomForest.v1 \nTask for estimating  nmse  using\n 5 x 10 - Fold Cross Validation\n     Run with seed =  1234 \nIteration :**************************************************\n\n\n++ MODEL/WORKFLOW :: randomForest.v2 \nTask for estimating  nmse  using\n 5 x 10 - Fold Cross Validation\n     Run with seed =  1234 \nIteration :**************************************************\n\n\n++ MODEL/WORKFLOW :: randomForest.v3 \nTask for estimating  nmse  using\n 5 x 10 - Fold Cross Validation\n     Run with seed =  1234 \nIteration :**************************************************\n\n\n\nrankWorkflows(res_all, top=3)\n\n$a1\n$a1$nmse\n         Workflow  Estimate\n1 randomForest.v2 0.5515591\n2 randomForest.v1 0.5517816\n3 randomForest.v3 0.5520359\n\n\n$a2\n$a2$nmse\n         Workflow  Estimate\n1 randomForest.v2 0.7806102\n2 randomForest.v3 0.7809335\n3 randomForest.v1 0.7832804\n\n\n$a3\n$a3$nmse\n         Workflow  Estimate\n1 randomForest.v3 0.9804490\n2 randomForest.v2 0.9843171\n3 randomForest.v1 0.9892036\n\n\n$a4\n$a4$nmse\n         Workflow  Estimate\n1 randomForest.v1 0.9326347\n2 randomForest.v2 0.9432211\n3 randomForest.v3 0.9433387\n\n\n$a5\n$a5$nmse\n         Workflow  Estimate\n1 randomForest.v3 0.7842300\n2 randomForest.v2 0.7852252\n3 randomForest.v1 0.7874437\n\n\n$a6\n$a6$nmse\n         Workflow  Estimate\n1              lm 0.8664797\n2 randomForest.v2 0.8988809\n3 randomForest.v3 0.9003739\n\n\n$a7\n$a7$nmse\n         Workflow Estimate\n1     rpartXse.v2 1.000000\n2     rpartXse.v3 1.000000\n3 randomForest.v3 1.016322\n\n\n\np &lt;- pairedComparisons(res_all, baseline=\"randomForest.v3\")\np$nmse$F.test\n\n$chi\n[1] 23.44898\n\n$FF\n[1] 7.584158\n\n$critVal\n[1] 0.6524015\n\n$rejNull\n[1] TRUE\n\np$nmse$BonferroniDunn.test\n\n$critDif\n[1] 3.046397\n\n$baseline\n[1] \"randomForest.v3\"\n\n$rkDifs\n             lm     rpartXse.v1     rpartXse.v2     rpartXse.v3 randomForest.v1 \n      2.5714286       4.1428571       2.5714286       2.2857143       0.7142857 \nrandomForest.v2 \n      0.2857143 \n\n$signifDifs\n             lm     rpartXse.v1     rpartXse.v2     rpartXse.v3 randomForest.v1 \n          FALSE            TRUE           FALSE           FALSE           FALSE \nrandomForest.v2 \n          FALSE",
    "crumbs": [
      "Métodos convencionales",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Modelos clásicos</span>"
    ]
  },
  {
    "objectID": "05-modelos-clasicos.html#predicción-de-valores",
    "href": "05-modelos-clasicos.html#predicción-de-valores",
    "title": "5  Modelos clásicos",
    "section": "5.5 Predicción de valores",
    "text": "5.5 Predicción de valores\nFinalmente, se ofrecen varias alternativas de evaluación de los modelos propuestos, centrado en la utilización del paquete performanceEstimation desarrollado por el mismo autor (Torgo, 2014).\nLa última sección ofrece pasos detallados para calcular las predicciones del modelo seleccionado, utilizando el testing dataset de 140 casos adicionales. Podemos encontrar también el código en la sección “Predictions for Seven Algae” del documento de este caso de estudio.\n\n\n\n\nTorgo, L. (2016). Data Mining with R: Learning with Case Studies, Second Edition. CRC Press. https://books.google.es/books?id=2aedDQAAQBAJ",
    "crumbs": [
      "Métodos convencionales",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Modelos clásicos</span>"
    ]
  },
  {
    "objectID": "05-modelos-clasicos.html#footnotes",
    "href": "05-modelos-clasicos.html#footnotes",
    "title": "5  Modelos clásicos",
    "section": "",
    "text": "Más información sobre los paquetes que componen el tidyverse disponible en http://tidyverse.org/.↩︎",
    "crumbs": [
      "Métodos convencionales",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Modelos clásicos</span>"
    ]
  },
  {
    "objectID": "06-modelos-avanzados.html",
    "href": "06-modelos-avanzados.html",
    "title": "6  Modelos avanzados",
    "section": "",
    "text": "6.1 Gradient Boosting\nEl Capítulo 12 de (Boehmke & Greenwell, 2019) explica en detalle ejemplos prácticos de entrenamiento y evaluación de modelos de aprendizaje de tipo Gradient Boosting Machines (GBM). La Figura 6.1 muestra de forma esquemática el proceso de ajuste y obtención de predicciones, mediante una aproximación de construcción secuencial de mejora de los weak learners individuales.\nLa Figura 6.2 muestra un ejemplo de ajuste iterativo de un modelo GBM para un problema de regresión, en el que se observa como la secuencia de ajuste va aproximando de forma cada vez más precisa un modelo de predicción GBM (rojo) sobre la verdadera función curvilínea (azul) que se ha empleado para generar los datos sintéticos (al fondo, en negro).\nUna de las variantes más potentes y empleadas hoy en día, debido a su flexibilidad y capacidad de computación distribuida, es XGBoost. Se explica en detalle en la Sec. 12.5 de (Boehmke & Greenwell, 2019).",
    "crumbs": [
      "Métodos convencionales",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Modelos avanzados</span>"
    ]
  },
  {
    "objectID": "06-modelos-avanzados.html#gradient-boosting",
    "href": "06-modelos-avanzados.html#gradient-boosting",
    "title": "6  Modelos avanzados",
    "section": "",
    "text": "Figura 6.1: Proceso secuencial de entrenamiento de un modelo de aprendizaje de tipo GBM. Fuente: Fig. 12.1 (Boehmke & Greenwell, 2019).\n\n\n\n\n\n\n\n\n\n\nFigura 6.2: Secuencia de etapas de ajuste de un modelo GBM para regresión (rojo) sobre la función real de generación de los datos simulados (azul). Fuente: Fig. 12.2 (Boehmke & Greenwell, 2019).",
    "crumbs": [
      "Métodos convencionales",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Modelos avanzados</span>"
    ]
  },
  {
    "objectID": "06-modelos-avanzados.html#aprendizaje-profundo",
    "href": "06-modelos-avanzados.html#aprendizaje-profundo",
    "title": "6  Modelos avanzados",
    "section": "6.2 Aprendizaje profundo",
    "text": "6.2 Aprendizaje profundo\nEl Capítulo 13 de (Boehmke & Greenwell, 2019) desarrolla varios ejemplos de redes neuronales multicapa (deep learning o aprendizaje profundo). Este tipo de modelos enfocan el problema del aprendizaje mapeando las características de entrada a los valores de una o varias variables de salida a través de estas capas, basándose en transformaciones de datos simples y en señales de retroalimentación.\n\n6.2.1 Dependencias software\nSe necesita una infraestructura hardware y software de cierta complejidad para poder entrenar y resolver este tipo de modelos. Normalmente, el primer paso en el diseño de nuestro sistema es seleccionar un entorno de trabajo o framework que nos ofrezca soporte para su creación, entrenamiento y validación. Hay muchas opciones disponibles en la actualidad, aunque todas ellas utilizan primordialmente el lenguaje de programación Python.\n\nTensorFlow: Es una librería software desarrollada por Google Brain, que permite implementar muchas tareas de IA y aprendizaje automático, pero que está centrada especialmente en entreamiento e inferencia de redes neuronales. Su API de programación usa el lenguaje Python, aunque internamente está escrita en C++ y también emplea bibliotecas de programación CUDA para utilizar nucleos tensores y otros recursos hardware de GPUs, para acelerar el cómputo.\nPyTorch: Es otra librería software muy habitual en desarrollo de redes neuronales multicapa para aprendizaje profundo. Incialmente desarrollada por Meta, cuenta en la actualidad con el respaldo de la Linux Foundation. Al igual que la anterior, ofrece una interfaz de programación en Python, aunque internamente está escrita en C++ y también aprovecha a bajo nivel las bibliotecas de programación CUDA para aceleración de cómputo con GPUs.\nKeras: Es otra herramienta software muy popular para desarrollo de este tipo de modelos de aprendizaje automático. Aunque durante bastante tiempo estuvo estrechamente ligada al proyecto TensorFlow, su versión 3.0 levanta esa restricción, ya que está reescrita para poder utilizar varios “motores” de bajo nivel, como PyTorch, JAX o TensorFlow, usando la misma base de código.\n\nEs recomendable leer con detenimiento y preparar las instrucciones de instalación de todo el stack software necesario con antelación. Además, si tienes pensado utilizar CUDA y una GPU para acelerar el cómputo de estos modelos, es muy recomendable estudiar con atención las recomendaciones sobre cómo instalar las dependencias necesarias en cada proyecto. Por ejemplo, Keras 3 incluye instrucciones específicas sobre el entorno GPU y cómo instalar las dependencias software necesarias en cada caso.\n\n\n\n\nBoehmke, B., & Greenwell, B. (2019). Hands-On Machine Learning with R (1.ª ed.). Chapman; Hall/CRC. https://doi.org/10.1201/9780367816377",
    "crumbs": [
      "Métodos convencionales",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Modelos avanzados</span>"
    ]
  },
  {
    "objectID": "07-tidymodels.html",
    "href": "07-tidymodels.html",
    "title": "7  Tidymodels",
    "section": "",
    "text": "7.1 Flujo de trabajo con Tidymodels",
    "crumbs": [
      "Entornos de modelado",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Tidymodels</span>"
    ]
  },
  {
    "objectID": "07-tidymodels.html#otro-paquete",
    "href": "07-tidymodels.html#otro-paquete",
    "title": "7  Tidymodels",
    "section": "7.2 Otro paquete",
    "text": "7.2 Otro paquete",
    "crumbs": [
      "Entornos de modelado",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Tidymodels</span>"
    ]
  },
  {
    "objectID": "07-tidymodels.html#preparación-de-datos-con-recipes",
    "href": "07-tidymodels.html#preparación-de-datos-con-recipes",
    "title": "7  Tidymodels",
    "section": "7.3 Preparación de datos con recipes",
    "text": "7.3 Preparación de datos con recipes",
    "crumbs": [
      "Entornos de modelado",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Tidymodels</span>"
    ]
  },
  {
    "objectID": "07-tidymodels.html#ajuste-de-modelos-con",
    "href": "07-tidymodels.html#ajuste-de-modelos-con",
    "title": "7  Tidymodels",
    "section": "7.4 Ajuste de modelos con…",
    "text": "7.4 Ajuste de modelos con…",
    "crumbs": [
      "Entornos de modelado",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Tidymodels</span>"
    ]
  },
  {
    "objectID": "07-tidymodels.html#evaluación-de-modelos-con-yardstick",
    "href": "07-tidymodels.html#evaluación-de-modelos-con-yardstick",
    "title": "7  Tidymodels",
    "section": "7.5 Evaluación de modelos con yardstick",
    "text": "7.5 Evaluación de modelos con yardstick",
    "crumbs": [
      "Entornos de modelado",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Tidymodels</span>"
    ]
  },
  {
    "objectID": "08-mlr3.html",
    "href": "08-mlr3.html",
    "title": "8  MLR3",
    "section": "",
    "text": "8.1 Ecosistema moderno de aprendizaje automático en R\nmlr3 es la reescritura moderna del popular paquete mlr. A diferencia de su predecesor, está construido sobre el sistema de programación orientada a objetos R6, lo que le confiere mayor velocidad, eficiencia en el manejo de memoria y una estructura modular extensible. Su filosofía se basa en unificar la interfaz para cientos de algoritmos de aprendizaje automático, permitiendo al usuario cambiar de modelo con una sintaxis estandarizada.",
    "crumbs": [
      "Entornos de modelado",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>MLR3</span>"
    ]
  },
  {
    "objectID": "08-mlr3.html#arquitectura-y-componentes-principales",
    "href": "08-mlr3.html#arquitectura-y-componentes-principales",
    "title": "8  MLR3",
    "section": "8.2 Arquitectura y componentes principales",
    "text": "8.2 Arquitectura y componentes principales\nLa figura Figura 8.1 muestra las diferentes conexiones entre los paquetes que constituyen este framework de desarrollo.\n\n\n\n\n\n\nFigura 8.1: Organización y dependencias del conjunto de paquetes MLR3. Fuente: Sitio web oficial de mlr3: https://mlr3.mlr-org.com/.\n\n\n\nEl flujo de trabajo en mlr3 se basa en la interacción de objetos R6. Para construir un modelo, debemos considerar varios tipos de “elementos de construcción”.\n\n8.2.1 Tarea\nRepresentada por el tipo Task. Es el punto de partida. Encapsula los datos (el data.frame o tibble) y define el objetivo del aprendizaje.\n\nBackend: los datos en sí (pueden ser locales o bases de datos SQL remotas).\nTarget: la variable que queremos predecir.\nTipos: marcan el objetivo, TaskClassif (clasificación) o TaskRegr (regresión).\n\n\n\n8.2.2 Aprendiz (learner)\nRepresentado por el objeto de tipo Learner. Es el algoritmo que va a aprender de los datos. mlr3 no implementa los algoritmos desde cero, sino que actúa como un wrapper (envoltorio) unificado alrededor de otros paquetes de R (como ranger, xgboost o glmnet).\nSe identifican mediante una cadena de texto, por ejemplo: \"classif.rpart\" o \"regr.lm\".\n\n\n8.2.3 Proceso de entrenamiento y predicción\nElementos de tipo Train o Predict.\n\nTrain: el método train() recibe una Task y genera un modelo entrenado (almacenando los parámetros aprendidos).\nPredict: el método predict() toma nuevos datos y devuelve un objeto Prediction, que alberga tanto los valores verdaderos como los predichos, facilitando la evaluación.\n\n\n\n8.2.4 Validación mediante remustreo\nObjetos de tipo Resampling. Define cómo validar el modelo para asegurar que generaliza bien (evitar overfitting).\n\nEstrategias comunes: validación Cruzada (\"cv\"), hold-out (\"holdout\"),bootstrap (\"bootstrap\").\nSe ejecuta mediante la función resample().\n\n\n\n8.2.5 Prueba o medida final\nEncapsulada en un objeto Measure. Es la métrica utilizada para cuantificar cómo de bueno es el modelo.\n\nEjemplos: Precisión (\"classif.acc\"), RMSE (\"regr.rmse\"), etc.",
    "crumbs": [
      "Entornos de modelado",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>MLR3</span>"
    ]
  },
  {
    "objectID": "08-mlr3.html#modelos-de-aprendizaje-supervisado-soportados",
    "href": "08-mlr3.html#modelos-de-aprendizaje-supervisado-soportados",
    "title": "8  MLR3",
    "section": "8.3 Modelos de aprendizaje supervisado soportados",
    "text": "8.3 Modelos de aprendizaje supervisado soportados\n\n8.3.1 Clasificación\nIdentificadores de tipo classif.*.\nUtilizados cuando la variable objetivo es categórica (binaria o multiclase).\n\nÁrboles de Decisión: classif.rpart (paquete rpart). Ya hemos visto que son interpretables y sencillos.\nBosques Aleatorios (Random Forests): classif.ranger (paquete ranger). Robustos y de alto rendimiento.\nMáquinas de Vector Soporte (SVM): classif.svm (paquete e1071). Efectivos en espacios de alta dimensionalidad.\nGradient Boosting: classif.xgboost (paquete xgboost) o classif.lightgbm. Se han convertido en una de las soluciones más populares en competiciones de aprendizaje automático.\nRegresión Logística: classif.log_reg (paquete stats). El modelo base estadístico clásico.\nK-Vecinos más Cercanos (\\(k-NN\\)): classif.kknn. Como hemos visto, basado en distancias.\nNaive Bayes: classif.naive_bayes. Probabilístico y sencillo de ajustar.\n\n\n\n8.3.2 Regresión\n\nRegresión Lineal: regr.lm (paquete stats). Simple y explicativo.\nRegresión Penalizada (Lasso/Ridge/ElasticNet): regr.glmnet. Ideal para selección de variables y evitar sobreajuste.\nÁrboles de Regresión: regr.rpart.\nBosques Aleatorios de Regresión: regr.ranger.\nGradient Boosting de Regresión: regr.xgboost.\nMáquinas de Vector Soporte (SVR): regr.svm.\nKriging: regr.km. Para datos geoespaciales o interpolación.\n\nRespecto a los algoritmos de aprendizaje profundo, aunque mlr3 se centra en datos estructurados, existe integración con mlr3keras o mlr3torch para redes neuronales profundas, aunque estos son ecosistemas más complejos de instalar y configurar adecuadamente.",
    "crumbs": [
      "Entornos de modelado",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>MLR3</span>"
    ]
  },
  {
    "objectID": "08-mlr3.html#principales-extensiones-del-ecosistema",
    "href": "08-mlr3.html#principales-extensiones-del-ecosistema",
    "title": "8  MLR3",
    "section": "8.4 Principales extensiones del ecosistema",
    "text": "8.4 Principales extensiones del ecosistema\nLa Tabla 8.1 muestra algunas de las extensiones más interesantes de este ecosistema, para poder sacarle el máximo partido.\n\n\n\nTabla 8.1: Extensiones clave del ecosistema mlr3\n\n\n\n\n\n\n\n\n\nPaquete\nFuncionalidad\n\n\n\n\nmlr3viz\nGenera visualizaciones rápidas y elegantes (curvas ROC, gráficos de predicción, análisis de residuos) basándose en el motor gráfico de ggplot2.\n\n\nmlr3tuning\nImplementa estrategias de optimización de hiperparámetros (Tuning), incluyendo Grid Search, Random Search y optimización bayesiana.\n\n\nmlr3pipelines\nPermite la creación de grafos de flujo de datos para pre-procesamiento (imputación, escalado, one-hot encoding) y ensamblado de modelos (Stacking).\n\n\nmlr3benchmark\nFacilita la comparación rigurosa y sistemática del rendimiento de múltiples modelos sobre múltiples tareas simultáneamente.",
    "crumbs": [
      "Entornos de modelado",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>MLR3</span>"
    ]
  },
  {
    "objectID": "09-add-resources.html",
    "href": "09-add-resources.html",
    "title": "9  Recursos adicionales",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n\n\n\nKnuth, D. E. (1984). Literate Programming. Comput. J., 27(2), 97-111. https://doi.org/10.1093/comjnl/27.2.97",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Recursos adicionales</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Referencias",
    "section": "",
    "text": "Agresti, A. (2015). Foundations of Linear and\nGeneralized Linear Models (1st ed.). John Wiley & Sons.\n\n\nBoehmke, B., & Greenwell, B. (2019). Hands-on machine learning\nwith r (1st ed.). Chapman; Hall/CRC. https://doi.org/10.1201/9780367816377\n\n\nBreiman, L. (2001). Random forests. Machine Learning,\n45(1), 5–32.\n\n\nDe Cock, D. (2011). Ames, iowa: Alternative to the boston housing data\nas an end of semester regression project. Journal of Statistics\nEducation, 19(3). https://doi.org/10.1080/10691898.2011.11889627\n\n\nEfron, B., & Tibshirani, R. J. (1993). Bootstrap Methods and Their Application.\nChapman; Hall/CRC.\n\n\nFawcett, T. (2006). An introduction to ROC analysis. Pattern\nRecognition Letters, 27(8), 861–874. https://doi.org/10.1016/j.patrec.2005.10.010\n\n\nGorman, R. P., & Sejnowski, T. J. (1988). Analysis of hidden units\nin a layered network trained to classify sonar targets. Neural\nNetworks, 1, 75–89. https://doi.org/10.1016/0893-6080(88)90023\n\n\nGrigorev, A. (2021). Machine learning bookcamp (1st ed.).\nManning Publications Co. https://www.manning.com/books/machine-learning-bookcamp\n\n\nHand, T., D. J. (2001). A simple generalization of the area under the\nROC curve to multiple class classification problems. Machine\nLearning, 45(2), 171–186. https://doi.org/10.1023/A:1010920819831\n\n\nHyndman, R. J., & Athanasopoulos, G. (2021). Forecasting: Principles and Practice (3rd\ned.). OTexts. https://otexts.com/fpp3/\n\n\nJames, W., G. (2021). An Introduction to\nStatistical Learning with Applications in R (2nd ed.).\nSpringer. https://www.statlearning.com/\n\n\nKnuth, D. E. (1984). Literate programming. Comput. J.,\n27(2), 97–111. https://doi.org/10.1093/comjnl/27.2.97\n\n\nKuhn, M. (2020). AmesHousing: The ames iowa housing data. https://doi.org/10.32614/CRAN.package.AmesHousing\n\n\nKuhn, M., & Johnson, K. (2013). Applied Predictive\nModeling. Springer New York. https://books.google.es/books?id=xYRDAAAAQBAJ\n\n\nKuhn, M., & Johnson, K. (2019). Feature\nEngineering and Selection: A Practical Approach for Predictive\nModels. Chapman & Hall/CRC. http://www.feat.engineering/\n\n\nKuncheva, L. I. (2014). Combining pattern classifiers: Methods and\nalgorithms (2nd ed.). Wiley-Interscience. https://lucykuncheva.co.uk/Combining_Pattern_Classifiers_Methods_and_Algorithms_2nd_ed_Kuncheva%202014-09-09.pdf\n\n\nLeisch, F., & Dimitriadou, E. (2024). Mlbench: Machine learning\nbenchmark problems. https://CRAN.R-project.org/package=mlbench\n\n\nRaschka, S. (2020). Model evaluation, model selection, and algorithm\nselection in machine learning. https://arxiv.org/abs/1811.12808\n\n\nRussell, S., & Norvig, P. (2009). Artificial intelligence: A\nmodern approach (3rd ed.). Prentice Hall Press.\n\n\nTorgo, L. (2016). Data Mining with R:\nLearning with Case Studies,\nSecond Edition. CRC Press. https://books.google.es/books?id=2aedDQAAQBAJ\n\n\nWolpert, D. H. (1996). The lack of a priori distinctions between\nlearning algorithms. Neural Computation, 8(7),\n1341–1390. https://doi.org/10.1162/neco.1996.8.7.1341\n\n\nZheng, A., & Casari, A. (2018). FFeature\nEngineering for Machine Learning: Principles and Techniques for Data\nScientists. O’Reilly Media, Inc. https://www.oreilly.com/library/view/feature-engineering-for/9781491953235/",
    "crumbs": [
      "Referencias"
    ]
  }
]