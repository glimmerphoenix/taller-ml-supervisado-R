# Selección y validación de modelos

## Consideraciones generales

Hace varias décadas, la evaluación y selección de modelos se efectuaba mediante pruebas estadísticas que medían
la bondad de los ajustes y a través de la evaluación de los residuos (errores) del modelo propuesto. Hoy en
días, la evaluación se realiza empleando medios mucho más robustos, las conocidas como funciones de pérdida
(*loss functions*).

Una **función de pérdida** es una métrica que compara los valores predichos respecto de los valores
que realmente se han producido. Su salida se denomina habitualmente **error** o bien **pseudo-residuo**. En
este contexto, tenemos que tener en cuenta una premisa fundamental:

$$
\text{DATOS} = \text{MODELO} + \text{ERROR}.
$$

- Los datos describen la realidad que queremos explicar mediante el modelo, a través de una serie de variables
que describen dicho problema.
- El modelo es una representación *simplificada* de la realidad, propuesta para intentar describirla de manera
más fácilmente comprensible. Cuanto más sencillo sea el modelo más fácil será interpretarlo, pero también puede
que cometa más imprecisiones al describir el fenómeno real que estudiamos.
- El error representa el fallo que cometemos al simplificar la realidad mediante el modelo simplificado.

Existen muchos tipos de funciones de pérdida para evaluar el rendimiento de modelos predictivos, cada una con
sus ventajas e inconvenientes. Es importante recordar que cada función de pérdida se calcula teniendo en cuenta
uno o varios tipos de error cometidos por el modelo, dejando otros de lado, por lo que es complicado afirmar
mirando una sola de estas funciones que podemos escoger un *modelo óptimo*. Dependiendo del problema en cuestión,
debemos seleccionar primero la función o funciones de pérdida más adecuadas para evaluar las características del
modelo que nos interesan. Después, podemos aplicarlas para evaluar los candidatos y seleccionar una opción.

## Métricas de rendimiento


### Modelos de regresión

- **Error Cuadrático Medio** (*Mean Squared Error* o MSE): el objetivo es *minimizar* el promedio de la
combinación de los errores cuadráticos cometidos en la predicción de cada punto.

$$
\text{MSE} = \frac{1}{n}\sum_{i=1}^n (y_i - \hat{y}_i)^2
$$ {#eq-RMSE}

Hay que tener en cuenta que en modelos de regresión lineal, el divisor de la ecuación @eq-RMSE es $n-p$ para
proporcionar una mejor estimación (reduciendo el sesgo).

- **Raíz del Error Cuadrático Medio** (*Root Mean Squared Error* o RMSE): es la raíz cuadrada del MSE. Su fin es
dar una estimación del error cometido en las mismas unidades que la variable de salida que buscamos predecir.
El objetivo aquí también es *minimizar* la función de pérdida.

- **Desviación**: desviación promedio de los residuos, cuando el modelo se estima mediante un método de
máxima verosimilitud. Se suele emplear en modelos de clasificación y el objetivo es *minimizar* su valor.

- **Error Medio Absoluto** (*Mean Absolute Error* o MAE): en este caso, en lugar de tomar la raíz cuadrada
del MSE se calculan las diferencias entre los valores reales y predichos tomando su valor absoluto, lo que
contiene la influencia de los errores con valores altos. EL objetivo es *minimizar* la función de pérdida.

- **Error Medio Cuadrático Logarítmico** (*Root Mean Squared Logarithmic Error** o RMSLE): Es la raíz cuadrada
del error medio cuadrático logarítmico. Permite que los errores grandes cometidos sobre valores de salida de
magnitud pequeña contribuyan de igual forma a la función de pérdida que los errores grandes cometidos sobre
valores de salida de magnitud grande, ya que de otro modo los segundos prevalecerían sobre los primeros. Aquí
el objetivo es *minimizar* igualmente la función de pérdida.

$$
\text{RMSLE} = \sqrt{\frac{1}{n} \sum_{i=1}^n (log(y_i +1) -  log(\hat{y}_i + 1))^2}.
$$ {#eq-rmsle}

- $R^2$ y $R_{adj}^2$: son métricas que representan el porcentaje de la varianza total de los datos que el
modelo es capaz de explicar. La versión ajustada intenta tener en cuenta el número de variables predictoras
empleadas, penalizando modelos más complejos. De otro modo, usar un mayor número de variables normalmente
aumentaría sin más el valor de $R^2$. El objetivo aquí es *maximizar* su valor. No obstante, es preciso obrar 
con precaución puesto que, como se indica en [@boehmke2019] (Sec. 2.6.2), dos modelos entrenados con datos
diferentes que tengan el mismo RMSE, pero en los que el primero tenga menos variabilidad (dispersión) en los
valores de la salida del modelo que el segundo tendrá también un $R^2$ menor.

### Modelos de clasificación {#sec-eval-classification}

Las métricas de rendimiento de los modelos de clasificación se suelen construir a partir de los valores de la
llamada **matriz de confusión**, que refleja el tipo de aciertos y errores cometidos por el modelo al clasificar,
como muesgtra la @fig-confusion-matrix.

![Representación de los diferentes tipos de aciertos y errores que refleja la matriz de confusión y el nombre que suelen recibir en evaluación de modelos de clasificación. Fuente: Fig. 2.12 [@boehmke2019].](img/confusion-matrix.png){#fig-confusion-matrix }

- **Accuracy**: se define como (objetivo: maximizar)

$$
\text{ACC} = \frac{\text{TP}+\text{TN}}{\text{total}}.
$$ {#eq-acc}

- **Precision**: se define como (objetivo: maximizar)

$$
\text{PREC} = \frac{\text{TP}}{\text{TP}+\text{FP}}.
$$ {#eq-prec}

- **Recall** (*sensitivity*): se define como (objetivo: maximizar)

$$
\text{RECALL}= \frac{\text{TP}}{\text{TP}+\text{FN}}.
$$ {#eq-recall}

- **Specificity**: se define como (objetivo: maximizar)

$$
\text{SPEC}=\frac{\text{TN}}{\text{TN}+\text{FP}}.
$$ {#eq-spec}

- **F1-score**: se trata de la media armónica entre precisión y *recall*, dado por

$$
\text{F1} = 2\;\frac{\text{PREC}\times\text{RECALL}}{\text{PREC} + \text{RECALL}}
$$ {#eq-f1}

- **F-score generalizado**: como el anterior, pero añadiendo pesos de ponderación, dado por

$$
\text{F1} = (1+\beta^2)\;\frac{\text{PREC}\times\text{RECALL}}{\beta^2\text{PREC} + \text{RECALL}}
$$ {#eq-f1-gral}

Si $\beta=1$ tenemos el F1-score. Si $\beta=2$ otorgamos el doble de peso a la recuperación frente a
la precisión y al contrario si $\beta=0.5$.

- **AUC** (Área Bajo la Curva ROC): dependiendo de los valores otorgados a los hiperparámetros del modelo, podremos mejorar la precisión,
la sensitividad o ambas, minimizando los falsos positivos y falsos negativos. Las curvas ROC (*Receiver
Operating Characteristic*) se crearon para represntar gráficamente estos valores en función de cada combinación
de los hiperparámetros del modelo, permitiendo una evaluación gráfica más general de su rendimiento. En este
caso el *Area Under the [ROC] Curve* (concretamente, entre la curva ROC y la línea recta diagonal que 
representa a un clasificador aleatorio) debe ser lo más grande posible. Estos conceptos se muestran en la
@fig-ROC-modelling.

![Representación de varias curvas ROC y su interpretación en evaluación de modelos de clasificación. Fuente: Fig. 2.14 [@boehmke2019].](img/ROC-modelling.png){#fig-ROC-modelling width="95%"}

Además de estas métricas, habituales en modelos de clasificación, podemos considerar otras posibles funciones
de pérdida.

- Error de clasificación: Es el error promedio total, que buscamos *minimizar*. Por ejemplo, asumamos 
tenemos que clasificar la salida en dos posibles grupos y cada uno consta de 40 observaciones. Si fallamos
al clasificar 6 elementos del primer grupo y 3 del segundo, en total hemos fallado en 9 de las 40 
observaciones, lo que implica un ratio de clasificación incorrecta del 11,25%.

- Error promedio por clase: Este es el error promedio cometido dentro de cada clase. Por ejemplo, en el
problema anterior sería $6/40$ en el primer grupo y $3/40$ en el segundo grupo. El propósito es minimizar
este error.

- Entropía cruzada (*cross-entropy* o *Log Loss* o *deviance*): esta métrica penaliza en gran medida los 
casos en los que se asigna una alta probabilidad a una salida incorrecta (es decir, el modelo se dice muy 
seguro de la respuesta, pero ha fallado). El objetivo también es minimizar su valor.

- Índice de Gini: empleado en modelos basados en árboles de decisión, es una medida de *pureza* (*purity*),
donde valores pequeños indican que un nodo contiene, de forma predominante, observaciones de una sola clase. Por tanto, permite medir lo bien que es capaz de separar las clases entre sí el árbol en cada nodo. El objetivo es minimizar su valor.

### Ajuste de parámetros

Como se ha explicado en la @sec-cross-validation, cuando escogemos una o varias métricas de evaluación de
rendimiento, el procedimiento que solemos seguir es aplicar un método de validación (cruzada con $k$
conjuntos, *bootstrapping*) para obtener un resultado de evaluación en cada iteración. Finalmente, se 
combinan estos resultados para obtener una evaluación final de cada modelo, así como unos valores
adecuados con los que fijar los hiperparámetros necesarios del modelo.

En el caso de los modelos de regresión (salida numérica), este proceso es bastante directo, comparando
los resultados de la métrica o métricas seleccionadas. Sin embargo, en el caso de modelos de clasificación
la evaluación es más complicada, porque como hemos visto hay que tener en cuenta el rendimiento del modelo
para cada uno de los posibles grupos o clases presentes en la variable de salida. Por ese motivo, y debido
al hecho de que diferentes combinaciones de los hiperparámetros del modelo ofrecen resultados distintos
para la sensibilidad y la especificidad, se prefiere optar por comparar modelos mediante las curvas
ROC.

En ciertas ocasiones, valdrá la pena, si tenemos tiempo y suficientes recursos computacionales a nuestra
disposición, realizar una búsqueda sistemática de los posibles valores de los hiperparámetros de nuestro
modelo para garantizar nuestra selección. Esta búsqueda sistemática o *grid search* implica definir un
rango de posibles valores entre los cuales realizaremos un "barrido", comparando los resultados obtenidos
para intentar conseguir un resultado adecuado.

## Comparación de modelos

### Curvas ROC

ROC convex HULL @fig-ROC-convex-hull [@fawcett2006]

![Representación del *convex hull* bajo varias curvas ROC, que determina posibles clasificadores óptimos para un problema dado, en función de las restricciones y prioridades consideradas. Fuente: Fig. 7 [@fawcett2006].](img/ROC-convex-hull.jpg){#fig-ROC-convex-hull width="100%"}